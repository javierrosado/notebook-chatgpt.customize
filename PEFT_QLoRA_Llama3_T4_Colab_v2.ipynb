{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeb8158",
   "metadata": {},
   "source": [
    "\n",
    "# PEFT / QLoRA **(Colab · Python 3 · GPU T4)** — Llama 3.x Instruct · v2\n",
    "\n",
    "Notebook actualizado para **Colab con CUDA 12.6**: incluye correcciones de instalación para **bitsandbytes** (rueda con soporte CUDA actual) y **Triton**, y mantiene ajustes de memoria/precisión para **T4 (16 GB)**.\n",
    "\n",
    "**Objetivo**: adaptar un modelo **Llama 3.x Instruct** a un **ChatGPT especializado en Arquitectura de Software** mediante **PEFT (LoRA/IA3/AdaLoRA)** y **QLoRA**.\n",
    "\n",
    "> Marcadores pedagógicos: **[TRANSFORMER]** indica dónde se usa la arquitectura Transformer. **[DATA TRANSFORM]** indica operaciones de transformación de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ee1c2",
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0) Instalación robusta para Colab (CUDA 12.6 / T4) — EJECUTA PRIMERO\n",
    "# ============================================================\n",
    "# Limpieza de paquetes opcionales que suelen causar conflictos y bnb previo\n",
    "# !pip -q uninstall -y flash-attn xformers bitsandbytes || true\n",
    "\n",
    "# Pila base fijada (estable) para evitar regresiones en Colab\n",
    "# !pip -q install -U \"transformers==4.45.2\" \"accelerate==0.34.2\" #   \"datasets==2.20.0\" \"peft==0.13.2\" \"trl==0.11.4\" \"sentencepiece==0.2.0\"\n",
    "\n",
    "# bitsandbytes con binarios recientes (incluye CUDA 12.x)\n",
    "# !pip -q install -U --pre bitsandbytes\n",
    "\n",
    "# Triton requerido por kernels/integraciones (alineado con PyTorch 2.5.x en Colab)\n",
    "# !pip -q install \"triton>=3.0.0\"\n",
    "\n",
    "# (Opcional) Si quieres volver a instalar xformers:\n",
    "# !pip -q install xformers\n",
    "# (Opcional) flash-attn suele ser innecesario en T4, pero si insistes:\n",
    "# !pip -q install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b96ea",
   "metadata": {
    "tags": [
     "diagnostic"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1) Verificación de entorno + bitsandbytes (CUDA 12.6)\n",
    "# ============================================================\n",
    "import torch, platform, sys, os, glob\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
    "    bnblibs = glob.glob(os.path.join(os.path.dirname(bnb.__file__), \"libbitsandbytes_cuda*.so\"))\n",
    "    print(\"BNB libs:\", bnblibs)\n",
    "    if not bnblibs:\n",
    "        print(\"⚠️  No se encontraron binarios CUDA de bitsandbytes. Considera reiniciar runtime y re-ejecutar la celda 0.\")\n",
    "except Exception as e:\n",
    "    print(\"bitsandbytes import error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ba0a3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 2) Configuración global (optimizada para T4 · 16 GB)\n",
    "# ============================================================\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BASE_MODEL: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    DATASET_LOCAL_JSONL: str = \"/content/datasets/arqsoft_chat.jsonl\"\n",
    "    DATASET_HF_ID: str | None = None\n",
    "    OUTPUT_DIR: str = \"/content/outputs/llama3_arqsoft_peft\"\n",
    "    ADAPTER_NAME: str = \"arqsoft-qlora\"\n",
    "    MAX_STEPS: int = 500\n",
    "    NUM_EPOCHS: int = 1\n",
    "    LEARNING_RATE: float = 2e-4\n",
    "    PER_DEVICE_BATCH_SIZE: int = 1\n",
    "    GRADIENT_ACCUMULATION: int = 16\n",
    "    MAX_SEQ_LEN: int = 1024\n",
    "    WARMUP_RATIO: float = 0.03\n",
    "    LOGGING_STEPS: int = 10\n",
    "    EVAL_STEPS: int = 100\n",
    "    SAVE_STEPS: int = 200\n",
    "    USE_BF16: bool = False\n",
    "    USE_FP16: bool = True\n",
    "    BNB_4BIT_COMPUTE_DTYPE: str = \"float16\"\n",
    "    LOAD_IN_4BIT: bool = True\n",
    "    BNB_4BIT_QUANT_TYPE: str = \"nf4\"\n",
    "    GRADIENT_CHECKPOINTING: bool = True\n",
    "    LORA_R: int = 16\n",
    "    LORA_ALPHA: int = 32\n",
    "    LORA_DROPOUT: float = 0.05\n",
    "    TARGET_MODULES: tuple[str, ...] = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
    "    TASK_TYPE: str = \"CAUSAL_LM\"\n",
    "    MAX_NEW_TOKENS: int = 256\n",
    "    TEMPERATURE: float = 0.2\n",
    "    TOP_P: float = 0.95\n",
    "\n",
    "CFG = Config()\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c495ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3) (Opcional) Monta Google Drive si tus datos están allí\n",
    "# ============================================================\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c135ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 4) Login a Hugging Face (necesario para descargar Llama 3.x)\n",
    "# ============================================================\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"hf_...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5) Carga Tokenizer y Modelo 4-bit (QLoRA)\n",
    "#    [TRANSFORMER] Aquí se instancia el Transformer Llama 3.x\n",
    "# ============================================================\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CFG.LOAD_IN_4BIT,\n",
    "    bnb_4bit_quant_type=CFG.BNB_4BIT_QUANT_TYPE,\n",
    "    bnb_4bit_compute_dtype=getattr(torch, CFG.BNB_4BIT_COMPUTE_DTYPE),\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.BASE_MODEL,\n",
    "    use_fast=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae090029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6) Preparación PEFT (LoRA sobre QLoRA)\n",
    "#    [TRANSFORMER] Inyectamos adaptadores LoRA en q/k/v/o y MLP\n",
    "# ============================================================\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=CFG.LORA_R,\n",
    "    lora_alpha=CFG.LORA_ALPHA,\n",
    "    lora_dropout=CFG.LORA_DROPOUT,\n",
    "    target_modules=list(CFG.TARGET_MODULES),\n",
    "    task_type=CFG.TASK_TYPE,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20f7ab",
   "metadata": {},
   "source": [
    "\n",
    "> **Alternativas PEFT**: IA3/AdaLoRA (cambia `peft_config`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7) Carga de Dataset (formato chat)\n",
    "# ============================================================\n",
    "from datasets import load_dataset, Dataset\n",
    "import json, os\n",
    "\n",
    "def load_chat_dataset(local_path: str | None, hf_id: str | None):\n",
    "    if local_path and os.path.exists(local_path):\n",
    "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            records = [json.loads(line) for line in f]\n",
    "        return Dataset.from_list(records)\n",
    "    elif hf_id:\n",
    "        return load_dataset(hf_id, split=\"train\")\n",
    "    else:\n",
    "        mini = [\n",
    "            {\"messages\": [\n",
    "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
    "                {\"role\":\"user\",\"content\":\"Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.\"}\n",
    "            ]},\n",
    "            {\"messages\": [\n",
    "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
    "                {\"role\":\"user\",\"content\":\"Diseña un patrón EDA en Kafka para fidelización al 99.99%.\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"Particiones y RF≥3, acks=all, min.insync.replicas=2, DLQ, Schema Registry, idempotent producer, outbox, SLO/SLI y alertas por latencia/lag.\"}\n",
    "            ]},\n",
    "        ]\n",
    "        return Dataset.from_list(mini)\n",
    "\n",
    "raw_ds = load_chat_dataset(CFG.DATASET_LOCAL_JSONL, CFG.DATASET_HF_ID)\n",
    "print(\"Ejemplo:\", raw_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf15fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 8) Transformación de datos\n",
    "#    [DATA TRANSFORM] chat_template → tokenización → labels (pad→-100)\n",
    "# ============================================================\n",
    "def format_and_tokenize(example):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=CFG.MAX_SEQ_LEN,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    if input_ids and isinstance(input_ids[0], list):\n",
    "        labels = [\n",
    "            [tok if tok != pad_id else -100 for tok in seq]\n",
    "            for seq in input_ids\n",
    "        ]\n",
    "    else:\n",
    "        labels = [tok if tok != pad_id else -100 for tok in input_ids]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "processed_ds = raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names)\n",
    "split = processed_ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 9) Entrenamiento (TRL SFTTrainer)\n",
    "# ============================================================\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import default_data_collator\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=CFG.OUTPUT_DIR,\n",
    "    max_seq_length=CFG.MAX_SEQ_LEN,\n",
    "    per_device_train_batch_size=CFG.PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=CFG.GRADIENT_ACCUMULATION,\n",
    "    learning_rate=CFG.LEARNING_RATE,\n",
    "    logging_steps=CFG.LOGGING_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CFG.EVAL_STEPS,\n",
    "    save_steps=CFG.SAVE_STEPS,\n",
    "    bf16=CFG.USE_BF16,\n",
    "    fp16=CFG.USE_FP16,\n",
    "    warmup_ratio=CFG.WARMUP_RATIO,\n",
    "    max_steps=CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else None,\n",
    "    num_train_epochs=None if CFG.MAX_STEPS > 0 else CFG.NUM_EPOCHS,\n",
    "    gradient_checkpointing=CFG.GRADIENT_CHECKPOINTING,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.model.save_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\")\n",
    "# tokenizer.save_pretrained(CFG.OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 10) Inferencia de prueba\n",
    "# ============================================================\n",
    "import torch\n",
    "\n",
    "def chat(prompt: str, sys: str = \"Eres un asistente experto en Arquitectura de Software.\"):\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": sys},\n",
    "        {\"role\":\"user\",\"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
    "            temperature=CFG.TEMPERATURE,\n",
    "            top_p=CFG.TOP_P,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(chat(\"Compara API Gateway vs Service Mesh.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeecd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 11) (Opcional) Merge del adaptador y exportación\n",
    "# ============================================================\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "# merged = AutoPeftModelForCausalLM.from_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\", device_map=\"auto\")\n",
    "# merged = merged.merge_and_unload()\n",
    "# merged.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\", safe_serialization=True)\n",
    "# tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a431da",
   "metadata": {},
   "source": [
    "\n",
    "## Troubleshooting (T4 + CUDA 12.6)\n",
    "- **bnb sin GPU / no lib cuda** → Repite la celda **0** y luego reinicia runtime. Verifica en **1)** que aparezca `libbitsandbytes_cuda126.so`.\n",
    "- **`bfloat16` no soportado** → Ya configurado (`USE_BF16=False`, `USE_FP16=True`).\n",
    "- **OOM** → Baja `MAX_SEQ_LEN` (1024→768/512), deja `BATCH=1`, mantén `GRADIENT_ACCUMULATION` alto, `gradient_checkpointing=True`.\n",
    "- **labels/pad** → Función de tokenización convierte PAD→`-100`.\n",
    "- **flash-attn/xformers** → Opcionales; SDPA de PyTorch es suficiente en T4.\n",
    "\n",
    "### Resumen didáctico\n",
    "- **[TRANSFORMER]**: celda **5** instancia `AutoModelForCausalLM` (Llama 3.x); **celda 6** inyecta LoRA en `q/k/v/o` y MLP.  \n",
    "- **[DATA TRANSFORM]**: celda **8** aplica `apply_chat_template` → `tokenizer` (trunc/pad) → `labels` (pad = -100).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}