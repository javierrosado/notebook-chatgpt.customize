{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eeb8158",
      "metadata": {
        "id": "4eeb8158"
      },
      "source": [
        "\n",
        "# PEFT / QLoRA **(Colab · Python 3 · GPU T4)** — Llama 3.x Instruct · v2\n",
        "\n",
        "Notebook actualizado para **Colab con CUDA 12.6**: incluye correcciones de instalación para **bitsandbytes** (rueda con soporte CUDA actual) y **Triton**, y mantiene ajustes de memoria/precisión para **T4 (16 GB)**.\n",
        "\n",
        "**Objetivo**: adaptar un modelo **Llama 3.x Instruct** a un **ChatGPT especializado en Arquitectura de Software** mediante **PEFT (LoRA/IA3/AdaLoRA)** y **QLoRA**.\n",
        "\n",
        "> Marcadores pedagógicos: **[TRANSFORMER]** indica dónde se usa la arquitectura Transformer. **[DATA TRANSFORM]** indica operaciones de transformación de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8ee1c2",
      "metadata": {
        "tags": [
          "install"
        ],
        "id": "df8ee1c2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0) Instalación robusta para Colab (CUDA 12.6 / T4) — EJECUTA PRIMERO\n",
        "# ============================================================\n",
        "# Limpieza de paquetes opcionales que suelen causar conflictos y bnb previo\n",
        "!pip uninstall -y flash-attn xformers bitsandbytes || true\n",
        "\n",
        "# Pila base fijada (estable) para evitar regresiones en Colab\n",
        "!pip install -U \"transformers==4.45.2\" \"accelerate==0.34.2\" #   \"datasets==2.20.0\" \"peft==0.13.2\" \"trl==0.11.4\" \"sentencepiece==0.2.0\"\n",
        "\n",
        "# bitsandbytes con binarios recientes (incluye CUDA 12.x)\n",
        "!pip install -U --pre bitsandbytes\n",
        "\n",
        "# Triton requerido por kernels/integraciones (alineado con PyTorch 2.5.x en Colab)\n",
        "!pip install \"triton>=3.0.0\"\n",
        "\n",
        "# (Opcional) Si quieres volver a instalar xformers:\n",
        "!pip install xformers\n",
        "# (Opcional) flash-attn suele ser innecesario en T4, pero si insistes:\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8b96ea",
      "metadata": {
        "tags": [
          "diagnostic"
        ],
        "id": "7c8b96ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 1) Verificación de entorno + bitsandbytes (CUDA 12.6)\n",
        "# ============================================================\n",
        "import torch, platform, sys, os, glob\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
        "    bnblibs = glob.glob(os.path.join(os.path.dirname(bnb.__file__), \"libbitsandbytes_cuda*.so\"))\n",
        "    print(\"BNB libs:\", bnblibs)\n",
        "    if not bnblibs:\n",
        "        print(\"⚠️  No se encontraron binarios CUDA de bitsandbytes. Considera reiniciar runtime y re-ejecutar la celda 0.\")\n",
        "except Exception as e:\n",
        "    print(\"bitsandbytes import error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501ba0a3",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "501ba0a3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2) Configuración global (optimizada para T4 · 16 GB)\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Ruta raíz para guardar/cargar archivos en Google Drive\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "\n",
        "    # Modelo base a usar de Hugging Face Hub\n",
        "    # Valores posibles: Cualquier modelo compatible con AutoModelForCausalLM (ej: \"meta-llama/Llama-3-8B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "    BASE_MODEL: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    # Ruta local al archivo JSONL con el dataset de chat\n",
        "    DATASET_LOCAL_JSONL: str = f\"{DRIVE_ROOT}/datasets/arqsoft_chat.jsonl\"\n",
        "\n",
        "    # ID del dataset en Hugging Face Hub (si se usa en lugar de local)\n",
        "    # Ejemplo: de ajibawa-2023/Software-Architecture, https://huggingface.co/datasets/ajibawa-2023/Software-Architecture\n",
        "    DATASET_HF_ID: str | None = None\n",
        "\n",
        "    # Directorio de salida para guardar checkpoints (punto de control del modelo) y el adaptador entrenado\n",
        "    OUTPUT_DIR: str = f\"{DRIVE_ROOT}/outputs/llama3_arqsoft_peft\"\n",
        "\n",
        "    # Nombre del adaptador PEFT (usado para guardar/cargar)\n",
        "    ADAPTER_NAME: str = \"arqsoft-qlora\"\n",
        "\n",
        "    # Número máximo de pasos de entrenamiento (iteraciones del optimizador)\n",
        "    # Valores posibles: Un entero positivo. -1 para entrenar por número de épocas.\n",
        "    MAX_STEPS: int = 10\n",
        "\n",
        "    # Número de épocas completas sobre el dataset de entrenamiento, el MAX_STEPS>1, esta variable no se toma en cuenta.\n",
        "    NUM_EPOCHS: int = 1\n",
        "\n",
        "    # Tasa de aprendizaje para el optimizador\n",
        "    # Valores posibles: Un flotante positivo pequeño (ej: 1e-5 a 5e-4)\n",
        "    LEARNING_RATE: float = 2e-4\n",
        "\n",
        "    # Tamaño del batch por dispositivo (GPU)\n",
        "    # Valores posibles: Un entero positivo (usualmente 1 para QLoRA en T4 para ahorrar memoria)\n",
        "    PER_DEVICE_BATCH_SIZE: int = 1\n",
        "\n",
        "    # Número de pasos de acumulación de gradiente\n",
        "    # Valores posibles: Un entero positivo. Batch efectivo = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
        "    GRADIENT_ACCUMULATION: int = 16\n",
        "\n",
        "    # Longitud máxima de secuencia para tokenizar (padding/truncation)\n",
        "    # Valores posibles: Un entero positivo. Depende del modelo y dataset (ej: 512, 1024, 2048)\n",
        "    MAX_SEQ_LEN: int = 1024\n",
        "\n",
        "    # Proporción de pasos usados para calentamiento (warmup) del learning rate\n",
        "    WARMUP_RATIO: float = 0.03\n",
        "\n",
        "    # Frecuencia para registrar métricas de entrenamiento (en pasos)\n",
        "    LOGGING_STEPS: int = 10\n",
        "\n",
        "    # Frecuencia para evaluar el modelo en el dataset de validación (en pasos)\n",
        "    # Valores posibles: Un entero positivo. Ignorado si eval_strategy=\"no\".\n",
        "    EVAL_STEPS: int = 100\n",
        "\n",
        "    # Frecuencia para guardar checkpoints del modelo (en pasos)\n",
        "    SAVE_STEPS: int = 200\n",
        "\n",
        "    # Usar precisión bfloat16 (requiere GPU Ampere+ y soporte)\n",
        "    # Valores posibles: bool (True/False). False para T4.\n",
        "    USE_BF16: bool = False\n",
        "\n",
        "    # Usar precisión float16 (más amplio soporte en GPUs como T4)\n",
        "    # Valores posibles: bool (True/False). True para T4.\n",
        "    USE_FP16: bool = True\n",
        "\n",
        "    # Tipo de dato para la computación en 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"float16\", \"bfloat16\" (si la GPU lo soporta)\n",
        "    BNB_4BIT_COMPUTE_DTYPE: str = \"float16\"\n",
        "\n",
        "    # Cargar el modelo en 4 bits usando bitsandbytes\n",
        "    LOAD_IN_4BIT: bool = True\n",
        "\n",
        "    # Tipo de cuantización de 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"nf4\", \"fp4\"\n",
        "    BNB_4BIT_QUANT_TYPE: str = \"nf4\"\n",
        "\n",
        "    # Habilitar gradient checkpointing para ahorrar memoria\n",
        "    # Valores posibles: bool (True/False). Recomendado en T4.\n",
        "    GRADIENT_CHECKPOINTING: bool = True\n",
        "\n",
        "    # Parámetro 'r' para LoRA: dimensión de los adaptadores\n",
        "    # Valores posibles: Un entero positivo (ej: 8, 16, 32, 64)\n",
        "    LORA_R: int = 16\n",
        "\n",
        "    # Parámetro 'lora_alpha' para LoRA: factor de escalado\n",
        "    # Valores posibles: Un entero positivo (ej: 16, 32, 64). Usualmente >= LORA_R.\n",
        "    LORA_ALPHA: int = 32\n",
        "\n",
        "    # Parámetro 'lora_dropout' para LoRA: dropout en los adaptadores\n",
        "    # Valores posibles: Un flotante entre 0.0 y 1.0\n",
        "    LORA_DROPOUT: float = 0.05\n",
        "\n",
        "    # Módulos del modelo base a los que se aplicará LoRA\n",
        "    TARGET_MODULES: tuple[str, ...] = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
        "\n",
        "    # Tipo de tarea para PEFT (Causal Language Modeling para generación de texto)\n",
        "    # Valores posibles: \"CAUSAL_LM\", \"SEQ_CLS\", etc.\n",
        "    TASK_TYPE: str = \"CAUSAL_LM\"\n",
        "\n",
        "    # Número máximo de tokens a generar durante la inferencia de prueba\n",
        "    MAX_NEW_TOKENS: int = 256\n",
        "\n",
        "    # Temperatura para el muestreo durante la generación (controla la aleatoriedad)\n",
        "    TEMPERATURE: float = 0.2\n",
        "\n",
        "    # Parámetro Top-P para el muestreo durante la generación (controla la diversidad)\n",
        "    TOP_P: float = 0.95\n",
        "\n",
        "CFG = Config()\n",
        "CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c495ef",
      "metadata": {
        "id": "e5c495ef"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3) Montar Google Drive para usar y almacenar datos\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Ruta raíz del Google Drive montado\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "# Carpeta para la caché de Hugging Face (modelos Llama 3.x, datasets (en caso se active su ID en Conf))\n",
        "HF_ROOT = f\"{DRIVE_ROOT}/hf_cache\"\n",
        "# Carpeta para almacenar datasets locales\n",
        "DATA_ROOT = f\"{DRIVE_ROOT}/datasets\"\n",
        "# Carpeta para guardar outputs del entrenamiento (checkpoints, adaptadores)\n",
        "OUT_ROOT = f\"{DRIVE_ROOT}/llama3_arqsoft_peft\"\n",
        "\n",
        "import os\n",
        "os.makedirs(HF_ROOT, exist_ok=True)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# Redirige caches de Hugging Face (modelos/datasets) a Drive\n",
        "os.environ[\"HF_HOME\"] = HF_ROOT        # raíz HF (recomendado)\n",
        "os.environ[\"HF_HUB_CACHE\"] = f\"{HF_ROOT}/hub\"   # opcional fino\n",
        "\n",
        "# Ajusta tu Config del notebook:\n",
        "CFG.DATASET_LOCAL_JSONL = f\"{DATA_ROOT}/arqsoft_chat.jsonl\"\n",
        "CFG.OUTPUT_DIR = OUT_ROOT\n",
        "\n",
        "# --- Agregar logs aquí ---\n",
        "print(\"\\n--- Rutas y configuraciones de Google Drive ---\")\n",
        "print(f\"DRIVE_ROOT (Raíz de Drive): {DRIVE_ROOT}\")\n",
        "print(f\"HF_ROOT (Caché de Hugging Face): {HF_ROOT}\")\n",
        "print(f\"DATA_ROOT (Datasets locales): {DATA_ROOT}\")\n",
        "print(f\"OUT_ROOT (Outputs de entrenamiento): {OUT_ROOT}\")\n",
        "print(f\"CFG.DATASET_LOCAL_JSONL (Ruta del dataset en Config): {CFG.DATASET_LOCAL_JSONL}\")\n",
        "print(f\"CFG.OUTPUT_DIR (Directorio de outputs en Config): {CFG.OUTPUT_DIR}\")\n",
        "print(\"---------------------------------------------\")\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Ruta del dataset configurada: {CFG.DATASET_LOCAL_JSONL}\")\n",
        "if os.path.exists(CFG.DATASET_LOCAL_JSONL):\n",
        "    print(f\"Archivo encontrado. Tamaño: {os.path.getsize(CFG.DATASET_LOCAL_JSONL)} bytes\")\n",
        "else:\n",
        "    print(\"Archivo NO encontrado en la ruta configurada.\")"
      ],
      "metadata": {
        "id": "jLjt6gG4FVVw"
      },
      "id": "jLjt6gG4FVVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c135ea",
      "metadata": {
        "id": "98c135ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 4) Login a Hugging Face (necesario para descargar Llama 3.x)\n",
        "# ============================================================\n",
        "# 4.1) Genera tu User Access Token en https://huggingface.co/settings/tokens (scope: \"read\" para descargar; \"write\" si vas a subir)\n",
        "# 4.2) En Colab: usa input seguro\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = getpass(\"HF token (no se mostrará): \")\n",
        "login(token=token)  # almacena el token en ~/.cache/huggingface\n",
        "\n",
        "from huggingface_hub import whoami\n",
        "print(whoami())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7208597e",
      "metadata": {
        "id": "7208597e"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5) Carga Tokenizer y Modelo 4-bit (QLoRA)\n",
        "#    [TRANSFORMER] Aquí se instancia el Transformer Llama 3.x\n",
        "# ============================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"--- Iniciando carga de Tokenizer y Modelo 4-bit (QLoRA) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Paso 1: Configurar BitsAndBytesConfig\n",
        "# Esta configuración especifica cómo se cargará y usará el modelo en 4 bits.\n",
        "print(\"Paso 1: Configurando BitsAndBytesConfig...\")\n",
        "# Input principal: Variables de configuración (LOAD_IN_4BIT, BNB_4BIT_QUANT_TYPE, BNB_4BIT_COMPUTE_DTYPE)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=CFG.LOAD_IN_4BIT,\n",
        "    bnb_4bit_quant_type=CFG.BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, CFG.BNB_4BIT_COMPUTE_DTYPE),\n",
        ")\n",
        "print(\"BitsAndBytesConfig configurado.\")\n",
        "# Output principal: Un objeto BitsAndBytesConfig\n",
        "print(f\"  BitsAndBytesConfig details: {bnb_config}\")\n",
        "\n",
        "\n",
        "# Paso 2: Cargar el Tokenizer\n",
        "# AutoTokenizer.from_pretrained carga el tokenizer asociado al modelo base especificado.\n",
        "# Es responsable de convertir texto en IDs numéricos (tokens) y viceversa.\n",
        "print(f\"Paso 2: Cargando Tokenizer desde {CFG.BASE_MODEL}...\")\n",
        "tokenizer_start_time = time.time()\n",
        "# Input principal: CFG.BASE_MODEL (string con el nombre/ruta del modelo)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    use_fast=True, # Usar la versión rápida del tokenizer (si está disponible)\n",
        "    padding_side=\"right\" # Configurar dónde añadir el padding (importante para modelos causales)\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Asegurar que haya un token de padding\n",
        "tokenizer_end_time = time.time()\n",
        "print(f\"Tokenizer cargado en {tokenizer_end_time - tokenizer_start_time:.2f} segundos.\")\n",
        "print(f\"Pad token configurado: {tokenizer.pad_token}, Pad token ID: {tokenizer.pad_token_id}\")\n",
        "# Output principal: Un objeto Tokenizer\n",
        "print(f\"  Tokenizer class: {type(tokenizer)}\")\n",
        "print(f\"  Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "\n",
        "# Paso 3: Cargar el Modelo en 4-bit\n",
        "# AutoModelForCausalLM.from_pretrained carga el modelo base pre-entrenado.\n",
        "# La cuantización BitsAndBytesConfig se aplica durante este proceso si LOAD_IN_4BIT es True.\n",
        "print(f\"Paso 3: Cargando Modelo 4-bit desde {CFG.BASE_MODEL}...de Hugging Face\")\n",
        "model_start_time = time.time()\n",
        "# Input principal: CFG.BASE_MODEL (string), quantization_config (objeto BitsAndBytesConfig), device_map\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    quantization_config=bnb_config, # Aplicar la configuración de cuantización\n",
        "    device_map=\"auto\", # Distribuir las capas del modelo automáticamente entre los dispositivos disponibles (GPU)\n",
        "    trust_remote_code=False # No ejecutar código arbitrario del Hub (generalmente True por seguridad)\n",
        ")\n",
        "model_end_time = time.time()\n",
        "print(f\"Modelo 4-bit cargado en {model_end_time - model_start_time:.2f} segundos.\")\n",
        "# Output principal: Un objeto AutoModelForCausalLM (cuantizado si se configuró así)\n",
        "print(f\"  Model class: {type(model)}\")\n",
        "print(f\"  Model device: {model.device}\") # Note: device_map=\"auto\" might show base device or a range\n",
        "\n",
        "\n",
        "# Configuraciones adicionales del modelo cargado\n",
        "model.config.use_cache = False # Deshabilitar caché durante el entrenamiento (ahorra memoria, útil para gradient checkpointing)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id # Asegurar que el modelo conozca el ID del token de padding\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Proceso de carga completado en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae090029",
      "metadata": {
        "id": "ae090029"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6) Preparación PEFT (LoRA sobre QLoRA)\n",
        "#    [TRANSFORMER] Inyectamos adaptadores LoRA en q/k/v/o y MLP\n",
        "# ============================================================\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import time # Import time for logging\n",
        "\n",
        "print(\"--- Iniciando preparación PEFT ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Paso 1: Preparar el modelo base cargado en 4-bit para entrenamiento k-bit (QLoRA)\n",
        "# Esta función aplica pre-procesamiento al modelo, como habilitar gradient checkpointing\n",
        "# si está configurado, y preparar los embeddings para el entrenamiento en 4-bit.\n",
        "print(\"Paso 1: Preparando modelo para entrenamiento k-bit...\")\n",
        "# Input principal: El objeto 'model' cargado en 4-bit de la celda anterior.\n",
        "print(f\"  Input a prepare_model_for_kbit_training: Model of type {type(model)}\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"Modelo preparado para entrenamiento k-bit.\")\n",
        "# Output principal: El mismo objeto 'model', pero con hooks y configuraciones para k-bit training.\n",
        "\n",
        "\n",
        "# Paso 2: Definir la configuración de LoRA\n",
        "# LoraConfig define los hiperparámetros del adaptador LoRA que se inyectará.\n",
        "# Estos hiperparámetros controlan el tamaño (r, lora_alpha) y la regularización (lora_dropout)\n",
        "# de las matrices de bajo rango, así como a qué módulos del modelo base se aplicarán (target_modules).\n",
        "print(\"Paso 2: Definiendo LoraConfig con los siguientes parámetros:\")\n",
        "print(f\"  r: {CFG.LORA_R}\")\n",
        "print(f\"  lora_alpha: {CFG.LORA_ALPHA}\")\n",
        "print(f\"  lora_dropout: {CFG.LORA_DROPOUT}\")\n",
        "print(f\"  target_modules: {CFG.TARGET_MODULES}\")\n",
        "print(f\"  task_type: {CFG.TASK_TYPE}\")\n",
        "peft_config = LoraConfig(\n",
        "    r=CFG.LORA_R,\n",
        "    lora_alpha=CFG.LORA_ALPHA,\n",
        "    lora_dropout=CFG.LORA_DROPOUT,\n",
        "    target_modules=list(CFG.TARGET_MODULES), # Aseguramos que sea una lista si target_modules es tupla\n",
        "    task_type=CFG.TASK_TYPE,\n",
        "    bias=\"none\" # Generalmente \"none\" para fine-tuning de modelos generativos\n",
        ")\n",
        "print(\"LoraConfig definido.\")\n",
        "# Output principal: Un objeto LoraConfig.\n",
        "print(f\"  Output de LoraConfig: {peft_config}\")\n",
        "\n",
        "\n",
        "# Paso 3: Obtener el modelo PEFT (inyectar los adaptadores LoRA)\n",
        "# Esta función toma el modelo base y la configuración LoRA, y devuelve\n",
        "# un \"PeftModel\", que es el modelo base con los adaptadores LoRA inyectados\n",
        "# y configurado para entrenar solo esos adaptadores.\n",
        "print(\"Paso 3: Inyectando adaptadores LoRA con get_peft_model...\")\n",
        "# Inputs: El modelo preparado para k-bit training y el objeto LoraConfig.\n",
        "print(f\"  Input 1 a get_peft_model: Model of type {type(model)}\")\n",
        "print(f\"  Input 2 a get_peft_model: LoraConfig of type {type(peft_config)}\")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\"Adaptadores LoRA inyectados. Modelo PEFT creado.\")\n",
        "# Output principal: Un objeto PeftModel.\n",
        "print(f\"  Output de get_peft_model: Model of type {type(model)}\")\n",
        "\n",
        "\n",
        "# Mostrar parámetros entrenables\n",
        "# Esto imprime cuántos parámetros totales tiene el modelo base y cuántos\n",
        "# parámetros adicionales (los de LoRA) serán entrenados.\n",
        "print(\"\\n--- Resumen de parámetros entrenables ---\")\n",
        "model.print_trainable_parameters()\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Preparación PEFT completada en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb20f7ab",
      "metadata": {
        "id": "bb20f7ab"
      },
      "source": [
        "\n",
        "> **Alternativas PEFT**: IA3/AdaLoRA (cambia `peft_config`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2f31f4",
      "metadata": {
        "id": "8f2f31f4"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7) Carga de Dataset (formato chat)\n",
        "# ============================================================\n",
        "from datasets import load_dataset, Dataset\n",
        "import json, os\n",
        "import time # Import time for logging\n",
        "\n",
        "print(\"--- Iniciando carga de Dataset ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Define una función para cargar el dataset desde un archivo local JSONL o desde Hugging Face Hub\n",
        "# Prioriza el archivo local si se especifica y existe.\n",
        "def load_chat_dataset(local_path: str | None, hf_id: str | None):\n",
        "    # --- Logs de entrada a la función ---\n",
        "    # Muestra los valores de los inputs local_path y hf_id que recibe la función.\n",
        "    print(f\"  Intentando cargar dataset desde: local_path='{local_path}', hf_id='{hf_id}'\")\n",
        "    # -----------------------------------\n",
        "    if local_path and os.path.exists(local_path):\n",
        "        print(f\"  Archivo local encontrado en: {local_path}. Cargando desde JSONL...\")\n",
        "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            # Lee cada línea como un objeto JSON (asumiendo que cada línea es un ejemplo de chat)\n",
        "            records = [json.loads(line) for line in f]\n",
        "        # Convierte la lista de diccionarios en un objeto Dataset de Hugging Face\n",
        "        dataset = Dataset.from_list(records)\n",
        "        print(f\"  Dataset cargado desde JSONL local. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "    elif hf_id:\n",
        "        print(f\"  Archivo local no encontrado o no especificado. Intentando cargar desde Hugging Face Hub: {hf_id}\")\n",
        "        # Carga el dataset directamente desde Hugging Face Hub\n",
        "        # Assume split 'train' by default, you might need to adjust this based on the dataset structure\n",
        "        dataset = load_dataset(hf_id, split=\"train\")\n",
        "        print(f\"  Dataset cargado desde Hugging Face Hub. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "    else:\n",
        "        print(\"  No se especificó archivo local ni ID de Hugging Face. Usando dataset mini de ejemplo.\")\n",
        "        # Si no se especifica ninguna fuente válida, crea un pequeño dataset de ejemplo\n",
        "        mini = [\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.\"}\n",
        "            ]},\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Diseña un patrón EDA en Kafka para fidelización al 99.99%.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"Particiones y RF≥3, acks=all, min.insync.replicas=2, DLQ, Schema Registry, idempotent producer, outbox, SLO/SLI y alertas por latencia/lag.\"}\n",
        "            ]},\n",
        "        ]\n",
        "        dataset = Dataset.from_list(mini)\n",
        "        print(f\"  Dataset mini de ejemplo creado. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "\n",
        "# Llama a la función para cargar el dataset utilizando las rutas/IDs de la configuración\n",
        "# Inputs a la función: CFG.DATASET_LOCAL_JSONL y CFG.DATASET_HF_ID definidos en la celda 2.\n",
        "raw_ds = load_chat_dataset(CFG.DATASET_LOCAL_JSONL, CFG.DATASET_HF_ID)\n",
        "\n",
        "# --- Log del primer ejemplo del dataset cargado ---\n",
        "# Muestra la estructura y contenido del primer ejemplo del dataset cargado (output de la función).\n",
        "print(\"\\n--- Primer ejemplo del Dataset cargado (Output de load_chat_dataset) ---\")\n",
        "print(raw_ds[0])\n",
        "print(\"---------------------------------------\")\n",
        "# --------------------------------------------\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Proceso de carga de Dataset completado en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf15fb0",
      "metadata": {
        "id": "fbf15fb0"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8) Transformación de datos\n",
        "#    [DATA TRANSFORM] chat_template → tokenización → labels (pad→-100)\n",
        "# ============================================================\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=CFG.MAX_SEQ_LEN,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    if input_ids and isinstance(input_ids[0], list):\n",
        "        labels = [\n",
        "            [tok if tok != pad_id else -100 for tok in seq]\n",
        "            for seq in input_ids\n",
        "        ]\n",
        "    else:\n",
        "        labels = [tok if tok != pad_id else -100 for tok in input_ids]\n",
        "\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names)\n",
        "\n",
        "# Divide el dataset procesado en conjuntos de entrenamiento y validación\n",
        "split = raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names).train_test_split(test_size=0.20, seed=42)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "# Muestra el número de ejemplos en cada conjunto\n",
        "print(f\"Dataset de Entrenamiento: {len(train_ds)} ejemplos\")\n",
        "print(f\"Dataset de Validación: {len(val_ds)} ejemplos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6681e6e5",
      "metadata": {
        "id": "6681e6e5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9) Entrenamiento (TRL SFTTrainer)\n",
        "# ============================================================\n",
        "try:\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "except ModuleNotFoundError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    print(\"Instalando TRL (trl==0.11.4)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl==0.11.4\"])\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "# Determine eval_strategy based on max_steps\n",
        "# Si MAX_STEPS > 0, la estrategia de evaluación se establece en \"no\" para evitar\n",
        "# evaluaciones intermedias basadas en pasos que podrían no alinearse bien con los checkpoints.\n",
        "# Si MAX_STEPS <= 0 (entrenamiento por épocas), la evaluación se basa en pasos (\"steps\").\n",
        "eval_strategy = \"steps\" if CFG.MAX_STEPS <= 0 else \"no\"\n",
        "\n",
        "# SFTConfig define los hiperparámetros y configuraciones para el entrenamiento de fine-tuning supervisado (SFT).\n",
        "# Es similar a TrainingArguments de la librería transformers, pero adaptada para SFT en TRL.\n",
        "print(\"--- Configurando SFTConfig para el entrenamiento ---\")\n",
        "# Inputs principales: Variables de configuración definidas en la clase Config (celda 2).\n",
        "print(f\"  Output directory: {CFG.OUTPUT_DIR}\")\n",
        "print(f\"  Max sequence length: {CFG.MAX_SEQ_LEN}\")\n",
        "print(f\"  Per device train batch size: {CFG.PER_DEVICE_BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation steps: {CFG.GRADIENT_ACCUMULATION}\")\n",
        "print(f\"  Learning rate: {CFG.LEARNING_RATE}\")\n",
        "print(f\"  Logging steps: {CFG.LOGGING_STEPS}\")\n",
        "print(f\"  Eval strategy: {eval_strategy}\")\n",
        "print(f\"  Eval steps: {CFG.EVAL_STEPS}\")\n",
        "print(f\"  Save steps: {CFG.SAVE_STEPS}\")\n",
        "print(f\"  Use BF16: {CFG.USE_BF16}\")\n",
        "print(f\"  Use FP16: {CFG.USE_FP16}\")\n",
        "print(f\"  Warmup ratio: {CFG.WARMUP_RATIO}\")\n",
        "print(f\"  Max steps (calculated): {CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1}\")\n",
        "print(f\"  Num train epochs (calculated): {CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000}\")\n",
        "print(f\"  Gradient checkpointing: {CFG.GRADIENT_CHECKPOINTING}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    max_seq_length=CFG.MAX_SEQ_LEN,\n",
        "    per_device_train_batch_size=CFG.PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=CFG.GRADIENT_ACCUMULATION,\n",
        "    learning_rate=CFG.LEARNING_RATE,\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    eval_strategy=eval_strategy, # Use the determined strategy\n",
        "    eval_steps=CFG.EVAL_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    bf16=CFG.USE_BF16,\n",
        "    fp16=CFG.USE_FP16,\n",
        "    warmup_ratio=CFG.WARMUP_RATIO,\n",
        "    max_steps=CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1, # Use -1 for no max steps\n",
        "    num_train_epochs=CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000, # Set epochs to a large value if max_steps is used\n",
        "    gradient_checkpointing=CFG.GRADIENT_CHECKPOINTING,\n",
        "    report_to=[\"none\"], # Deshabilita reportes a plataformas como Weights & Biases por defecto\n",
        ")\n",
        "print(\"SFTConfig configurado.\")\n",
        "# Output principal: Un objeto SFTConfig.\n",
        "\n",
        "\n",
        "# SFTTrainer es la clase principal de TRL para realizar el fine-tuning supervisado.\n",
        "# Abstrae el bucle de entrenamiento estándar, la preparación de datos, la optimización,\n",
        "# y la evaluación, especialmente optimizado para modelos grandes y PEFT.\n",
        "print(\"\\n--- Inicializando SFTTrainer ---\")\n",
        "# Inputs principales:\n",
        "# - model: El modelo PEFT preparado (de la celda 6).\n",
        "# - tokenizer: El tokenizer cargado (de la celda 5).\n",
        "# - args: El objeto SFTConfig con las configuraciones de entrenamiento.\n",
        "# - train_dataset: El dataset de entrenamiento procesado (de la celda 8).\n",
        "# - eval_dataset: El dataset de validación procesado (de la celda 8).\n",
        "# - data_collator: Una función para agrupar ejemplos del dataset en batches.\n",
        "print(f\"  Input model type: {type(model)}\")\n",
        "print(f\"  Input tokenizer type: {type(tokenizer)}\")\n",
        "#print(f\"  Input args (SFTConfig): {sft_config}\")\n",
        "print(f\"  Input train_dataset size: {len(train_ds)} ejemplos\")\n",
        "print(f\"  Input eval_dataset size: {len(val_ds)} ejemplos\")\n",
        "print(f\"  Input data_collator: {default_data_collator}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "print(\"SFTTrainer inicializado.\")\n",
        "# Output principal: Un objeto SFTTrainer.\n",
        "\n",
        "\n",
        "# Para iniciar el entrenamiento, descomenta la línea trainer.train()\n",
        "print(\"\\n--- Listo para iniciar el entrenamiento ---\")\n",
        "trainer.train()\n",
        "print(\"--- Entrenamiento finalizado ---\")\n",
        "\n",
        "# Para guardar el adaptador después del entrenamiento, descomenta estas líneas\n",
        "print(f\"\\n--- Guardando adaptador PEFT en {CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME} ---\")\n",
        "trainer.model.save_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\")\n",
        "tokenizer.save_pretrained(CFG.OUTPUT_DIR) # Guarda el tokenizer también\n",
        "print(\"--- Adaptador y tokenizer guardados ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53c70f7",
      "metadata": {
        "id": "f53c70f7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10) Inferencia de prueba\n",
        "# ============================================================\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "def chat(prompt: str, sys: str = \"Eres un asistente experto en Arquitectura de Software.\"):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\": sys},\n",
        "        {\"role\":\"user\",\"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # --- Agregar logs aquí ---\n",
        "    print(\"\\n--- Detalles de Inferencia ---\")\n",
        "    print(f\"Modelo base: {CFG.BASE_MODEL}\")\n",
        "    print(f\"Usando PEFT/LoRA: {isinstance(model, PeftModel)}\")\n",
        "    print(f\"Dispositivo del modelo: {model.device}\")\n",
        "    print(f\"Texto de entrada tokenizado: {text[:500]}...\") # Imprime los primeros 500 caracteres\n",
        "    print(f\"Longitud del texto de entrada: {len(text)}\")\n",
        "    # --------------------------\n",
        "\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # --- Más logs sobre los inputs ---\n",
        "    print(f\"Inputs tensor shape: {inputs['input_ids'].shape}\")\n",
        "    print(f\"Inputs tensor device: {inputs['input_ids'].device}\")\n",
        "    print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
        "    print(f\"Attention mask device: {inputs['attention_mask'].device}\")\n",
        "    print(f\"Max new tokens: {CFG.MAX_NEW_TOKENS}\")\n",
        "    print(f\"Temperature: {CFG.TEMPERATURE}\")\n",
        "    print(f\"Top P: {CFG.TOP_P}\")\n",
        "    print(\"-----------------------------\")\n",
        "    # -------------------------------\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
        "            temperature=CFG.TEMPERATURE,\n",
        "            top_p=CFG.TOP_P,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(chat(\"En terminos simples, explicame de que trata arquitectura dirigida de microservicios\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faeecd54",
      "metadata": {
        "id": "faeecd54"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11) (Opcional) Merge del adaptador y exportación\n",
        "# ============================================================\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import os\n",
        "import torch # Import torch\n",
        "\n",
        "# Define an offload directory (still needed if device_map is not used but model is large)\n",
        "offload_directory = \"/tmp/offload\"\n",
        "os.makedirs(offload_directory, exist_ok=True)\n",
        "\n",
        "# Determine device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "merged = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\",\n",
        "    # device_map=\"auto\", # Remove auto device mapping\n",
        "    # Use explicit device if needed, or rely on default\n",
        "    offload_folder=offload_directory # Keep offload directory as a fallback/option\n",
        ").to(device) # Explicitly move to device\n",
        "\n",
        "merged = merged.merge_and_unload()\n",
        "merged.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a431da",
      "metadata": {
        "id": "c3a431da"
      },
      "source": [
        "\n",
        "## Troubleshooting (T4 + CUDA 12.6)\n",
        "- **bnb sin GPU / no lib cuda** → Repite la celda **0** y luego reinicia runtime. Verifica en **1)** que aparezca `libbitsandbytes_cuda126.so`.\n",
        "- **`bfloat16` no soportado** → Ya configurado (`USE_BF16=False`, `USE_FP16=True`).\n",
        "- **OOM** → Baja `MAX_SEQ_LEN` (1024→768/512), deja `BATCH=1`, mantén `GRADIENT_ACCUMULATION` alto, `gradient_checkpointing=True`.\n",
        "- **labels/pad** → Función de tokenización convierte PAD→`-100`.\n",
        "- **flash-attn/xformers** → Opcionales; SDPA de PyTorch es suficiente en T4.\n",
        "\n",
        "### Resumen didáctico\n",
        "- **[TRANSFORMER]**: celda **5** instancia `AutoModelForCausalLM` (Llama 3.x); **celda 6** inyecta LoRA en `q/k/v/o` y MLP.  \n",
        "- **[DATA TRANSFORM]**: celda **8** aplica `apply_chat_template` → `tokenizer` (trunc/pad) → `labels` (pad = -100).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}