{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eeb8158",
      "metadata": {
        "id": "4eeb8158"
      },
      "source": [
        "\n",
        "# PEFT / QLoRA **(Colab · Python 3 · GPU T4)** — Llama 3.x Instruct · v2\n",
        "\n",
        "Notebook actualizado para **Colab con CUDA 12.6**: incluye correcciones de instalación para **bitsandbytes** (rueda con soporte CUDA actual) y **Triton**, y mantiene ajustes de memoria/precisión para **T4 (16 GB)**.\n",
        "\n",
        "**Objetivo**: adaptar un modelo **Llama 3.x Instruct** a un **ChatGPT especializado en Arquitectura de Software** mediante **PEFT (LoRA/IA3/AdaLoRA)** y **QLoRA**.\n",
        "\n",
        "> Marcadores pedagógicos: **[TRANSFORMER]** indica dónde se usa la arquitectura Transformer. **[DATA TRANSFORM]** indica operaciones de transformación de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df8ee1c2",
      "metadata": {
        "tags": [
          "install"
        ],
        "id": "df8ee1c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 0) Instalación robusta para Colab (CUDA 12.6 / T4) — EJECUTA PRIMERO\n",
        "# ============================================================\n",
        "# Limpieza de paquetes opcionales que suelen causar conflictos y bnb previo\n",
        "!pip -q uninstall -y flash-attn xformers bitsandbytes || true\n",
        "\n",
        "# Pila base fijada (estable) para evitar regresiones en Colab\n",
        "!pip -q install -U \"transformers==4.45.2\" \"accelerate==0.34.2\" #   \"datasets==2.20.0\" \"peft==0.13.2\" \"trl==0.11.4\" \"sentencepiece==0.2.0\"\n",
        "\n",
        "# bitsandbytes con binarios recientes (incluye CUDA 12.x)\n",
        "!pip -q install -U --pre bitsandbytes\n",
        "\n",
        "# Triton requerido por kernels/integraciones (alineado con PyTorch 2.5.x en Colab)\n",
        "!pip -q install \"triton>=3.0.0\"\n",
        "\n",
        "# (Opcional) Si quieres volver a instalar xformers:\n",
        "!pip -q install xformers\n",
        "# (Opcional) flash-attn suele ser innecesario en T4, pero si insistes:\n",
        "!pip -q install flash-attn --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c8b96ea",
      "metadata": {
        "tags": [
          "diagnostic"
        ],
        "id": "7c8b96ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e919ea-1062-451c-adfe-db9d3a5048fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126 | CUDA: 12.6 | CUDA available: True\n",
            "GPU: Tesla T4\n",
            "bitsandbytes: 0.48.2\n",
            "BNB libs: ['/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda130.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda125.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda124.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda126.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 1) Verificación de entorno + bitsandbytes (CUDA 12.6)\n",
        "# ============================================================\n",
        "import torch, platform, sys, os, glob\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
        "    bnblibs = glob.glob(os.path.join(os.path.dirname(bnb.__file__), \"libbitsandbytes_cuda*.so\"))\n",
        "    print(\"BNB libs:\", bnblibs)\n",
        "    if not bnblibs:\n",
        "        print(\"⚠️  No se encontraron binarios CUDA de bitsandbytes. Considera reiniciar runtime y re-ejecutar la celda 0.\")\n",
        "except Exception as e:\n",
        "    print(\"bitsandbytes import error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "501ba0a3",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "501ba0a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac278e4c-f4ba-4235-b29c-1eb3e2f46326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(BASE_MODEL='meta-llama/Llama-3.1-8B-Instruct', DATASET_LOCAL_JSONL='/content/datasets/arqsoft_chat.jsonl', DATASET_HF_ID=None, OUTPUT_DIR='/content/outputs/llama3_arqsoft_peft', ADAPTER_NAME='arqsoft-qlora', MAX_STEPS=500, NUM_EPOCHS=1, LEARNING_RATE=0.0002, PER_DEVICE_BATCH_SIZE=1, GRADIENT_ACCUMULATION=16, MAX_SEQ_LEN=1024, WARMUP_RATIO=0.03, LOGGING_STEPS=10, EVAL_STEPS=100, SAVE_STEPS=200, USE_BF16=False, USE_FP16=True, BNB_4BIT_COMPUTE_DTYPE='float16', LOAD_IN_4BIT=True, BNB_4BIT_QUANT_TYPE='nf4', GRADIENT_CHECKPOINTING=True, LORA_R=16, LORA_ALPHA=32, LORA_DROPOUT=0.05, TARGET_MODULES=('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'), TASK_TYPE='CAUSAL_LM', MAX_NEW_TOKENS=256, TEMPERATURE=0.2, TOP_P=0.95)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 2) Configuración global (optimizada para T4 · 16 GB)\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_MODEL: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    DATASET_LOCAL_JSONL: str = \"/content/datasets/arqsoft_chat.jsonl\"\n",
        "    DATASET_HF_ID: str | None = None\n",
        "    OUTPUT_DIR: str = \"/content/outputs/llama3_arqsoft_peft\"\n",
        "    ADAPTER_NAME: str = \"arqsoft-qlora\"\n",
        "    MAX_STEPS: int = 500\n",
        "    NUM_EPOCHS: int = 1\n",
        "    LEARNING_RATE: float = 2e-4\n",
        "    PER_DEVICE_BATCH_SIZE: int = 1\n",
        "    GRADIENT_ACCUMULATION: int = 16\n",
        "    MAX_SEQ_LEN: int = 1024\n",
        "    WARMUP_RATIO: float = 0.03\n",
        "    LOGGING_STEPS: int = 10\n",
        "    EVAL_STEPS: int = 100\n",
        "    SAVE_STEPS: int = 200\n",
        "    USE_BF16: bool = False\n",
        "    USE_FP16: bool = True\n",
        "    BNB_4BIT_COMPUTE_DTYPE: str = \"float16\"\n",
        "    LOAD_IN_4BIT: bool = True\n",
        "    BNB_4BIT_QUANT_TYPE: str = \"nf4\"\n",
        "    GRADIENT_CHECKPOINTING: bool = True\n",
        "    LORA_R: int = 16\n",
        "    LORA_ALPHA: int = 32\n",
        "    LORA_DROPOUT: float = 0.05\n",
        "    TARGET_MODULES: tuple[str, ...] = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
        "    TASK_TYPE: str = \"CAUSAL_LM\"\n",
        "    MAX_NEW_TOKENS: int = 256\n",
        "    TEMPERATURE: float = 0.2\n",
        "    TOP_P: float = 0.95\n",
        "\n",
        "CFG = Config()\n",
        "CFG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c495ef",
      "metadata": {
        "id": "e5c495ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 3) (Opcional) Monta Google Drive si tus datos están allí\n",
        "# ============================================================\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c135ea",
      "metadata": {
        "id": "98c135ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 4) Login a Hugging Face (necesario para descargar Llama 3.x)\n",
        "# ============================================================\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"hf_...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7208597e",
      "metadata": {
        "id": "7208597e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "825c1d187ac940d287f68298a65a0376",
            "ead7448069ba48778e1c3e74f06e141b",
            "67d1dc3e981f432cb8c1fc659b31c29b",
            "dc6264f84a084131a0057d48c6ad1ed2",
            "537e0ace9e3f476f9944b3234ba441b5",
            "1b3be1d5783f4447a1c7c221051c5cb2",
            "27455e7ec32a4500bf8a10ecd55bd63c",
            "4aebf43bdbec488e8e0291d7b9876a3a",
            "051340aab309402989f9916cbd18e8c0",
            "8f867a46075443a6a196bdcb0952b2e4",
            "fa758b41738142eda5470403ffc021ae"
          ]
        },
        "outputId": "667a5624-4559-4239-b95e-2d6a3d9bf016"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "825c1d187ac940d287f68298a65a0376"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 5) Carga Tokenizer y Modelo 4-bit (QLoRA)\n",
        "#    [TRANSFORMER] Aquí se instancia el Transformer Llama 3.x\n",
        "# ============================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=CFG.LOAD_IN_4BIT,\n",
        "    bnb_4bit_quant_type=CFG.BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, CFG.BNB_4BIT_COMPUTE_DTYPE),\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    use_fast=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ae090029",
      "metadata": {
        "id": "ae090029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c34d3a6-9193-43a6-e98a-2122c3b79584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 6) Preparación PEFT (LoRA sobre QLoRA)\n",
        "#    [TRANSFORMER] Inyectamos adaptadores LoRA en q/k/v/o y MLP\n",
        "# ============================================================\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=CFG.LORA_R,\n",
        "    lora_alpha=CFG.LORA_ALPHA,\n",
        "    lora_dropout=CFG.LORA_DROPOUT,\n",
        "    target_modules=list(CFG.TARGET_MODULES),\n",
        "    task_type=CFG.TASK_TYPE,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb20f7ab",
      "metadata": {
        "id": "bb20f7ab"
      },
      "source": [
        "\n",
        "> **Alternativas PEFT**: IA3/AdaLoRA (cambia `peft_config`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f2f31f4",
      "metadata": {
        "id": "8f2f31f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a552019-e813-40cd-b981-82228ea7fb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo: {'messages': [{'content': 'Eres un asistente experto en Arquitectura de Software.', 'role': 'system'}, {'content': 'Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.', 'role': 'user'}, {'content': 'API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.', 'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 7) Carga de Dataset (formato chat)\n",
        "# ============================================================\n",
        "from datasets import load_dataset, Dataset\n",
        "import json, os\n",
        "\n",
        "def load_chat_dataset(local_path: str | None, hf_id: str | None):\n",
        "    if local_path and os.path.exists(local_path):\n",
        "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            records = [json.loads(line) for line in f]\n",
        "        return Dataset.from_list(records)\n",
        "    elif hf_id:\n",
        "        return load_dataset(hf_id, split=\"train\")\n",
        "    else:\n",
        "        mini = [\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.\"}\n",
        "            ]},\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Diseña un patrón EDA en Kafka para fidelización al 99.99%.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"Particiones y RF≥3, acks=all, min.insync.replicas=2, DLQ, Schema Registry, idempotent producer, outbox, SLO/SLI y alertas por latencia/lag.\"}\n",
        "            ]},\n",
        "        ]\n",
        "        return Dataset.from_list(mini)\n",
        "\n",
        "raw_ds = load_chat_dataset(CFG.DATASET_LOCAL_JSONL, CFG.DATASET_HF_ID)\n",
        "print(\"Ejemplo:\", raw_ds[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fbf15fb0",
      "metadata": {
        "id": "fbf15fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "237f1e11a8814ae5ba00637241e52767",
            "1da07c5eb68e4e72be3a78e9a30a3b6f",
            "6a74533bf77b46be83bf94082282bea4",
            "05093fa2d0bc4a27a2ccc71b8f7a06a0",
            "dcb5008da6ca46cba8113c878803f2dd",
            "e4c4d23a38f644e5a2be3fcaecdf904a",
            "13cab9607b3a4cc0ad97a67990c3b1e1",
            "68d6ab4e61c74461b7f52ead2905663f",
            "8b43313e648c489b80b90e173b6b7768",
            "fb041cec156247a08040c78cd5eda843",
            "e74d01541a4e49f6a060c50bfb2b3f2a"
          ]
        },
        "outputId": "dfd91759-cfcc-44df-b1d6-afb8ff131e29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "237f1e11a8814ae5ba00637241e52767"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 8) Transformación de datos\n",
        "#    [DATA TRANSFORM] chat_template → tokenización → labels (pad→-100)\n",
        "# ============================================================\n",
        "def format_and_tokenize(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=CFG.MAX_SEQ_LEN,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    if input_ids and isinstance(input_ids[0], list):\n",
        "        labels = [\n",
        "            [tok if tok != pad_id else -100 for tok in seq]\n",
        "            for seq in input_ids\n",
        "        ]\n",
        "    else:\n",
        "        labels = [tok if tok != pad_id else -100 for tok in input_ids]\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "processed_ds = raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names)\n",
        "split = processed_ds.train_test_split(test_size=0.05, seed=42)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6681e6e5",
      "metadata": {
        "id": "6681e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d4aad1-3931-4c68-daaf-9b3bbc79d24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 9) Entrenamiento (TRL SFTTrainer)\n",
        "# ============================================================\n",
        "try:\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "except ModuleNotFoundError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    print(\"Instalando TRL (trl==0.11.4)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl==0.11.4\"])\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "# Determine eval_strategy based on max_steps\n",
        "eval_strategy = \"steps\" if CFG.MAX_STEPS <= 0 else \"no\" # Disable step evaluation if max_steps is used\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    max_seq_length=CFG.MAX_SEQ_LEN,\n",
        "    per_device_train_batch_size=CFG.PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=CFG.GRADIENT_ACCUMULATION,\n",
        "    learning_rate=CFG.LEARNING_RATE,\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    eval_strategy=eval_strategy, # Use the determined strategy\n",
        "    eval_steps=CFG.EVAL_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    bf16=CFG.USE_BF16,\n",
        "    fp16=CFG.USE_FP16,\n",
        "    warmup_ratio=CFG.WARMUP_RATIO,\n",
        "    max_steps=CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1, # Use -1 for no max steps\n",
        "    num_train_epochs=CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000, # Set epochs to a large value if max_steps is used\n",
        "    gradient_checkpointing=CFG.GRADIENT_CHECKPOINTING,\n",
        "    report_to=[\"none\"],\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "# trainer.train()\n",
        "# trainer.model.save_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\")\n",
        "# tokenizer.save_pretrained(CFG.OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f53c70f7",
      "metadata": {
        "id": "f53c70f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1bd159-97e2-4419-8526-14c73194572a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "Eres un asistente experto en Arquitectura de Software.user\n",
            "\n",
            "Compara API Gateway vs Service Mesh.assistant\n",
            "\n",
            "API Gateway y Service Mesh son dos tecnologías diferentes que se utilizan en la arquitectura de microservicios, pero que cumplen objetivos y funciones distintas.\n",
            "\n",
            "**API Gateway**\n",
            "\n",
            "Un API Gateway es un componente de software que actúa como una entrada única para acceder a múltiples servicios o microservicios detrás de él. Su función principal es recibir solicitudes de clientes externos, autenticarlas, autorizarlas y luego dirigirlas a los servicios correspondientes. El API Gateway puede realizar varias tareas, como:\n",
            "\n",
            "1. **Autenticación y autorización**: Verificar la identidad de los clientes y determinar qué servicios pueden acceder a qué recursos.\n",
            "2. **Caching**: Almacenar resultados de consultas frecuentes para mejorar el rendimiento.\n",
            "3. **Monitoreo y seguimiento**: Recopilar métricas y logs para analizar el rendimiento y la salud de los servicios.\n",
            "4. **Carga de trabajo**: Distribuir la carga de trabajo entre múltiples instancias de los servicios.\n",
            "\n",
            "**Service Mesh**\n",
            "\n",
            "Un Service Mesh es un conjunto de tecnologías que permiten la gestión y el control de la comunicación entre servicios en un entorno de micro\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 10) Inferencia de prueba\n",
        "# ============================================================\n",
        "import torch\n",
        "\n",
        "def chat(prompt: str, sys: str = \"Eres un asistente experto en Arquitectura de Software.\"):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\": sys},\n",
        "        {\"role\":\"user\",\"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
        "            temperature=CFG.TEMPERATURE,\n",
        "            top_p=CFG.TOP_P,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(chat(\"Compara API Gateway vs Service Mesh.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faeecd54",
      "metadata": {
        "id": "faeecd54"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 11) (Opcional) Merge del adaptador y exportación\n",
        "# ============================================================\n",
        "# from peft import AutoPeftModelForCausalLM\n",
        "# merged = AutoPeftModelForCausalLM.from_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\", device_map=\"auto\")\n",
        "# merged = merged.merge_and_unload()\n",
        "# merged.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\", safe_serialization=True)\n",
        "# tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a431da",
      "metadata": {
        "id": "c3a431da"
      },
      "source": [
        "\n",
        "## Troubleshooting (T4 + CUDA 12.6)\n",
        "- **bnb sin GPU / no lib cuda** → Repite la celda **0** y luego reinicia runtime. Verifica en **1)** que aparezca `libbitsandbytes_cuda126.so`.\n",
        "- **`bfloat16` no soportado** → Ya configurado (`USE_BF16=False`, `USE_FP16=True`).\n",
        "- **OOM** → Baja `MAX_SEQ_LEN` (1024→768/512), deja `BATCH=1`, mantén `GRADIENT_ACCUMULATION` alto, `gradient_checkpointing=True`.\n",
        "- **labels/pad** → Función de tokenización convierte PAD→`-100`.\n",
        "- **flash-attn/xformers** → Opcionales; SDPA de PyTorch es suficiente en T4.\n",
        "\n",
        "### Resumen didáctico\n",
        "- **[TRANSFORMER]**: celda **5** instancia `AutoModelForCausalLM` (Llama 3.x); **celda 6** inyecta LoRA en `q/k/v/o` y MLP.  \n",
        "- **[DATA TRANSFORM]**: celda **8** aplica `apply_chat_template` → `tokenizer` (trunc/pad) → `labels` (pad = -100).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "825c1d187ac940d287f68298a65a0376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ead7448069ba48778e1c3e74f06e141b",
              "IPY_MODEL_67d1dc3e981f432cb8c1fc659b31c29b",
              "IPY_MODEL_dc6264f84a084131a0057d48c6ad1ed2"
            ],
            "layout": "IPY_MODEL_537e0ace9e3f476f9944b3234ba441b5"
          }
        },
        "ead7448069ba48778e1c3e74f06e141b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3be1d5783f4447a1c7c221051c5cb2",
            "placeholder": "​",
            "style": "IPY_MODEL_27455e7ec32a4500bf8a10ecd55bd63c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "67d1dc3e981f432cb8c1fc659b31c29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aebf43bdbec488e8e0291d7b9876a3a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_051340aab309402989f9916cbd18e8c0",
            "value": 4
          }
        },
        "dc6264f84a084131a0057d48c6ad1ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f867a46075443a6a196bdcb0952b2e4",
            "placeholder": "​",
            "style": "IPY_MODEL_fa758b41738142eda5470403ffc021ae",
            "value": " 4/4 [01:26&lt;00:00, 18.58s/it]"
          }
        },
        "537e0ace9e3f476f9944b3234ba441b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3be1d5783f4447a1c7c221051c5cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27455e7ec32a4500bf8a10ecd55bd63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aebf43bdbec488e8e0291d7b9876a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "051340aab309402989f9916cbd18e8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f867a46075443a6a196bdcb0952b2e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa758b41738142eda5470403ffc021ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237f1e11a8814ae5ba00637241e52767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1da07c5eb68e4e72be3a78e9a30a3b6f",
              "IPY_MODEL_6a74533bf77b46be83bf94082282bea4",
              "IPY_MODEL_05093fa2d0bc4a27a2ccc71b8f7a06a0"
            ],
            "layout": "IPY_MODEL_dcb5008da6ca46cba8113c878803f2dd"
          }
        },
        "1da07c5eb68e4e72be3a78e9a30a3b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4c4d23a38f644e5a2be3fcaecdf904a",
            "placeholder": "​",
            "style": "IPY_MODEL_13cab9607b3a4cc0ad97a67990c3b1e1",
            "value": "Map: 100%"
          }
        },
        "6a74533bf77b46be83bf94082282bea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d6ab4e61c74461b7f52ead2905663f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b43313e648c489b80b90e173b6b7768",
            "value": 2
          }
        },
        "05093fa2d0bc4a27a2ccc71b8f7a06a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb041cec156247a08040c78cd5eda843",
            "placeholder": "​",
            "style": "IPY_MODEL_e74d01541a4e49f6a060c50bfb2b3f2a",
            "value": " 2/2 [00:00&lt;00:00, 30.33 examples/s]"
          }
        },
        "dcb5008da6ca46cba8113c878803f2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c4d23a38f644e5a2be3fcaecdf904a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13cab9607b3a4cc0ad97a67990c3b1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68d6ab4e61c74461b7f52ead2905663f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b43313e648c489b80b90e173b6b7768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb041cec156247a08040c78cd5eda843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e74d01541a4e49f6a060c50bfb2b3f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}