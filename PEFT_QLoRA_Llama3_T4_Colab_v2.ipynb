{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eeb8158",
      "metadata": {
        "id": "4eeb8158"
      },
      "source": [
        "\n",
        "# PEFT / QLoRA **(Colab · Python 3 · GPU T4)** — Llama 3.x Instruct · v2\n",
        "\n",
        "Notebook actualizado para **Colab con CUDA 12.6**: incluye correcciones de instalación para **bitsandbytes** (rueda con soporte CUDA actual) y **Triton**, y mantiene ajustes de memoria/precisión para **T4 (16 GB)**.\n",
        "\n",
        "**Objetivo**: adaptar un modelo **Llama 3.x Instruct** a un **ChatGPT especializado en Arquitectura de Software** mediante **PEFT (LoRA/IA3/AdaLoRA)** y **QLoRA**.\n",
        "\n",
        "> Marcadores pedagógicos: **[TRANSFORMER]** indica dónde se usa la arquitectura Transformer. **[DATA TRANSFORM]** indica operaciones de transformación de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df8ee1c2",
      "metadata": {
        "tags": [
          "install"
        ],
        "id": "df8ee1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c05e69-be30-4702-ddba-0d82ffe49875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flash_attn 2.8.3\n",
            "Uninstalling flash_attn-2.8.3:\n",
            "  Successfully uninstalled flash_attn-2.8.3\n",
            "Found existing installation: xformers 0.0.32.post2\n",
            "Uninstalling xformers-0.0.32.post2:\n",
            "  Successfully uninstalled xformers-0.0.32.post2\n",
            "Found existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "Requirement already satisfied: transformers==4.45.2 in /usr/local/lib/python3.12/dist-packages (4.45.2)\n",
            "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.12/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.3)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=3.0.0) (75.2.0)\n",
            "Collecting xformers\n",
            "  Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.3)\n",
            "Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.32.post2\n",
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) Instalación para Colab (CUDA 12.6 / T4) — EJECUTA PRIMERO\n",
        "# ============================================================\n",
        "# Limpieza de paquetes opcionales que suelen causar conflictos y bnb previo\n",
        "!pip uninstall -y flash-attn xformers bitsandbytes || true\n",
        "\n",
        "# Pila base fijada (estable) para evitar regresiones en Colab\n",
        "!pip install -U \"transformers==4.45.2\" \"accelerate==0.34.2\" #   \"datasets==2.20.0\" \"peft==0.13.2\" \"trl==0.11.4\" \"sentencepiece==0.2.0\"\n",
        "\n",
        "# bitsandbytes con binarios recientes (incluye CUDA 12.x)\n",
        "!pip install -U --pre bitsandbytes\n",
        "\n",
        "# Triton requerido por kernels/integraciones (alineado con PyTorch 2.5.x en Colab)\n",
        "!pip install \"triton>=3.0.0\"\n",
        "\n",
        "# (Opcional) Si quieres volver a instalar xformers:\n",
        "!pip install xformers\n",
        "# (Opcional) flash-attn suele ser innecesario en T4, pero si insistes:\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c8b96ea",
      "metadata": {
        "tags": [
          "diagnostic"
        ],
        "id": "7c8b96ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d76f467-df28-4577-cacf-05b372934f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126 | CUDA: 12.6 | CUDA available: True\n",
            "GPU: Tesla T4\n",
            "bitsandbytes: 0.48.2\n",
            "BNB libs: ['/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda130.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda126.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda125.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda124.so']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 1) Verificación de entorno + bitsandbytes (CUDA 12.6)\n",
        "# ============================================================\n",
        "import torch, platform, sys, os, glob\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
        "    bnblibs = glob.glob(os.path.join(os.path.dirname(bnb.__file__), \"libbitsandbytes_cuda*.so\"))\n",
        "    print(\"BNB libs:\", bnblibs)\n",
        "    if not bnblibs:\n",
        "        print(\"⚠️  No se encontraron binarios CUDA de bitsandbytes. Considera reiniciar runtime y re-ejecutar la celda 0.\")\n",
        "except Exception as e:\n",
        "    print(\"bitsandbytes import error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "501ba0a3",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "501ba0a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0415ab8-62f3-4fc0-f0e3-e16e3e5c47de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(BASE_MODEL='meta-llama/Llama-3.1-8B-Instruct', DATASET_LOCAL_JSONL='/content/drive/MyDrive/datasets/arqsoft_chat.jsonl', DATASET_HF_ID=None, OUTPUT_DIR='/content/drive/MyDrive/outputs/llama3_arqsoft_peft', ADAPTER_NAME='arqsoft-qlora', MAX_STEPS=200, NUM_EPOCHS=1, LEARNING_RATE=0.0002, PER_DEVICE_BATCH_SIZE=1, GRADIENT_ACCUMULATION=16, MAX_SEQ_LEN=1024, WARMUP_RATIO=0.03, LOGGING_STEPS=10, EVAL_STEPS=100, SAVE_STEPS=200, USE_BF16=False, USE_FP16=True, BNB_4BIT_COMPUTE_DTYPE='float16', LOAD_IN_4BIT=True, BNB_4BIT_QUANT_TYPE='nf4', GRADIENT_CHECKPOINTING=True, LORA_R=16, LORA_ALPHA=32, LORA_DROPOUT=0.05, TARGET_MODULES=('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'), TASK_TYPE='CAUSAL_LM', MAX_NEW_TOKENS=256, TEMPERATURE=0.2, TOP_P=0.95)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2) Configuración global (optimizada para T4 · 16 GB)\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Ruta raíz para guardar/cargar archivos en Google Drive\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "\n",
        "    # Modelo base a usar de Hugging Face Hub\n",
        "    # Valores posibles: Cualquier modelo compatible con AutoModelForCausalLM (ej: \"meta-llama/Llama-3-8B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "    BASE_MODEL: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    # Ruta local al archivo JSONL con el dataset de chat\n",
        "    DATASET_LOCAL_JSONL: str = f\"{DRIVE_ROOT}/datasets/arqsoft_chat.jsonl\"\n",
        "\n",
        "    # ID del dataset en Hugging Face Hub (si se usa en lugar de local)\n",
        "    # Ejemplo: de ajibawa-2023/Software-Architecture, https://huggingface.co/datasets/ajibawa-2023/Software-Architecture\n",
        "    DATASET_HF_ID: str | None = None\n",
        "\n",
        "    # Directorio de salida para guardar checkpoints (punto de control del modelo) y el adaptador entrenado\n",
        "    OUTPUT_DIR: str = f\"{DRIVE_ROOT}/outputs/llama3_arqsoft_peft\"\n",
        "\n",
        "    # Nombre del adaptador PEFT (usado para guardar/cargar)\n",
        "    ADAPTER_NAME: str = \"arqsoft-qlora\"\n",
        "\n",
        "    # Número máximo de pasos de entrenamiento (iteraciones del optimizador)\n",
        "    # Valores posibles: Un entero positivo. -1 para entrenar por número de épocas.\n",
        "    MAX_STEPS: int = 200\n",
        "\n",
        "    # Número de épocas completas sobre el dataset de entrenamiento, el MAX_STEPS>1, esta variable no se toma en cuenta.\n",
        "    NUM_EPOCHS: int = 1\n",
        "\n",
        "    # Tasa de aprendizaje para el optimizador\n",
        "    # Valores posibles: Un flotante positivo pequeño (ej: 1e-5 a 5e-4)\n",
        "    LEARNING_RATE: float = 2e-4\n",
        "\n",
        "    # Tamaño del batch por dispositivo (GPU)\n",
        "    # Valores posibles: Un entero positivo (usualmente 1 para QLoRA en T4 para ahorrar memoria)\n",
        "    PER_DEVICE_BATCH_SIZE: int = 1\n",
        "\n",
        "    # Número de pasos de acumulación de gradiente\n",
        "    # Valores posibles: Un entero positivo. Batch efectivo = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
        "    GRADIENT_ACCUMULATION: int = 16\n",
        "\n",
        "    # Longitud máxima de secuencia para tokenizar (padding/truncation)\n",
        "    # Valores posibles: Un entero positivo. Depende del modelo y dataset (ej: 512, 1024, 2048)\n",
        "    MAX_SEQ_LEN: int = 1024\n",
        "\n",
        "    # Proporción de pasos usados para calentamiento (warmup) del learning rate\n",
        "    WARMUP_RATIO: float = 0.03\n",
        "\n",
        "    # Frecuencia para registrar métricas de entrenamiento (en pasos)\n",
        "    LOGGING_STEPS: int = 10\n",
        "\n",
        "    # Frecuencia para evaluar el modelo en el dataset de validación (en pasos)\n",
        "    # Valores posibles: Un entero positivo. Ignorado si eval_strategy=\"no\".\n",
        "    EVAL_STEPS: int = 100\n",
        "\n",
        "    # Frecuencia para guardar checkpoints del modelo (en pasos)\n",
        "    SAVE_STEPS: int = 200\n",
        "\n",
        "    # Usar precisión bfloat16 (requiere GPU Ampere+ y soporte)\n",
        "    # Valores posibles: bool (True/False). False para T4.\n",
        "    USE_BF16: bool = False\n",
        "\n",
        "    # Usar precisión float16 (más amplio soporte en GPUs como T4)\n",
        "    # Valores posibles: bool (True/False). True para T4.\n",
        "    USE_FP16: bool = True\n",
        "\n",
        "    # Tipo de dato para la computación en 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"float16\", \"bfloat16\" (si la GPU lo soporta)\n",
        "    BNB_4BIT_COMPUTE_DTYPE: str = \"float16\"\n",
        "\n",
        "    # Cargar el modelo en 4 bits usando bitsandbytes\n",
        "    LOAD_IN_4BIT: bool = True\n",
        "\n",
        "    # Tipo de cuantización de 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"nf4\", \"fp4\"\n",
        "    BNB_4BIT_QUANT_TYPE: str = \"nf4\"\n",
        "\n",
        "    # Habilitar gradient checkpointing para ahorrar memoria\n",
        "    # Valores posibles: bool (True/False). Recomendado en T4.\n",
        "    GRADIENT_CHECKPOINTING: bool = True\n",
        "\n",
        "    # Parámetro 'r' para LoRA: dimensión de los adaptadores\n",
        "    # Valores posibles: Un entero positivo (ej: 8, 16, 32, 64)\n",
        "    LORA_R: int = 16\n",
        "\n",
        "    # Parámetro 'lora_alpha' para LoRA: factor de escalado\n",
        "    # Valores posibles: Un entero positivo (ej: 16, 32, 64). Usualmente >= LORA_R.\n",
        "    LORA_ALPHA: int = 32\n",
        "\n",
        "    # Parámetro 'lora_dropout' para LoRA: dropout en los adaptadores\n",
        "    # Valores posibles: Un flotante entre 0.0 y 1.0\n",
        "    LORA_DROPOUT: float = 0.05\n",
        "\n",
        "    # Módulos del modelo base a los que se aplicará LoRA\n",
        "    TARGET_MODULES: tuple[str, ...] = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
        "\n",
        "    # Tipo de tarea para PEFT (Causal Language Modeling para generación de texto)\n",
        "    # Valores posibles: \"CAUSAL_LM\", \"SEQ_CLS\", etc.\n",
        "    TASK_TYPE: str = \"CAUSAL_LM\"\n",
        "\n",
        "    # Número máximo de tokens a generar durante la inferencia de prueba\n",
        "    MAX_NEW_TOKENS: int = 256\n",
        "\n",
        "    # Temperatura para el muestreo durante la generación (controla la aleatoriedad)\n",
        "    TEMPERATURE: float = 0.2\n",
        "\n",
        "    # Parámetro Top-P para el muestreo durante la generación (controla la diversidad)\n",
        "    TOP_P: float = 0.95\n",
        "\n",
        "CFG = Config()\n",
        "CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5c495ef",
      "metadata": {
        "id": "e5c495ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae868234-a205-415f-d95c-d89c6f936935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "--- Rutas y configuraciones de Google Drive ---\n",
            "DRIVE_ROOT (Raíz de Drive): /content/drive/MyDrive\n",
            "HF_ROOT (Caché de Hugging Face): /content/drive/MyDrive/hf_cache\n",
            "DATA_ROOT (Datasets locales): /content/drive/MyDrive/datasets\n",
            "OUT_ROOT (Outputs de entrenamiento): /content/drive/MyDrive/llama3_arqsoft_peft\n",
            "CFG.DATASET_LOCAL_JSONL (Ruta del dataset en Config): /content/drive/MyDrive/datasets/arqsoft_chat.jsonl\n",
            "CFG.OUTPUT_DIR (Directorio de outputs en Config): /content/drive/MyDrive/llama3_arqsoft_peft\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 3) Montar Google Drive para usar y almacenar datos\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Ruta raíz del Google Drive montado\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "# Carpeta para la caché de Hugging Face (modelos Llama 3.x, datasets (en caso se active su ID en Conf))\n",
        "HF_ROOT = f\"{DRIVE_ROOT}/hf_cache\"\n",
        "# Carpeta para almacenar datasets locales\n",
        "DATA_ROOT = f\"{DRIVE_ROOT}/datasets\"\n",
        "# Carpeta para guardar outputs del entrenamiento (checkpoints, adaptadores)\n",
        "OUT_ROOT = f\"{DRIVE_ROOT}/llama3_arqsoft_peft\"\n",
        "\n",
        "import os\n",
        "os.makedirs(HF_ROOT, exist_ok=True)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# Redirige caches de Hugging Face (modelos/datasets) a Drive\n",
        "os.environ[\"HF_HOME\"] = HF_ROOT        # raíz HF (recomendado)\n",
        "os.environ[\"HF_HUB_CACHE\"] = f\"{HF_ROOT}/hub\"   # opcional fino\n",
        "\n",
        "# Ajusta tu Config del notebook:\n",
        "CFG.DATASET_LOCAL_JSONL = f\"{DATA_ROOT}/arqsoft_chat.jsonl\"\n",
        "CFG.OUTPUT_DIR = OUT_ROOT\n",
        "\n",
        "# --- Agregar logs aquí ---\n",
        "print(\"\\n--- Rutas y configuraciones de Google Drive ---\")\n",
        "print(f\"DRIVE_ROOT (Raíz de Drive): {DRIVE_ROOT}\")\n",
        "print(f\"HF_ROOT (Caché de Hugging Face): {HF_ROOT}\")\n",
        "print(f\"DATA_ROOT (Datasets locales): {DATA_ROOT}\")\n",
        "print(f\"OUT_ROOT (Outputs de entrenamiento): {OUT_ROOT}\")\n",
        "print(f\"CFG.DATASET_LOCAL_JSONL (Ruta del dataset en Config): {CFG.DATASET_LOCAL_JSONL}\")\n",
        "print(f\"CFG.OUTPUT_DIR (Directorio de outputs en Config): {CFG.OUTPUT_DIR}\")\n",
        "print(\"---------------------------------------------\")\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Ruta del dataset configurada: {CFG.DATASET_LOCAL_JSONL}\")\n",
        "if os.path.exists(CFG.DATASET_LOCAL_JSONL):\n",
        "    print(f\"Archivo encontrado. Tamaño: {os.path.getsize(CFG.DATASET_LOCAL_JSONL)} bytes\")\n",
        "else:\n",
        "    print(\"Archivo NO encontrado en la ruta configurada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLjt6gG4FVVw",
        "outputId": "28168c2a-efdb-4f95-ace3-6d6001a51061"
      },
      "id": "jLjt6gG4FVVw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta del dataset configurada: /content/drive/MyDrive/datasets/arqsoft_chat.jsonl\n",
            "Archivo encontrado. Tamaño: 2554275 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98c135ea",
      "metadata": {
        "id": "98c135ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc1be4a-38d8-4285-d81e-12b2790eae34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF token (no se mostrará): ··········\n",
            "{'type': 'user', 'id': '6850d845b8db7f9a70d8bc79', 'name': 'jrosado1974', 'fullname': 'Javier Rosado', 'email': 'javier.rosado@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SX44udlzqpt_Sw7i_nfo7.png', 'orgs': [{'type': 'org', 'id': '66a4f3f3f496b42dc0dd174c', 'name': 'LatinAI', 'fullname': 'AI Developers from Latin America', 'email': None, 'canPay': False, 'periodEnd': None, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65665c2af450504854d60806/l6qHJbnizngi_fnojAI2t.png', 'roleInOrg': 'contributor', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'maestria-uni', 'role': 'write', 'createdAt': '2025-11-06T01:36:23.776Z'}}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 4) Login a Hugging Face (necesario para descargar Llama 3.x)\n",
        "# ============================================================\n",
        "# 4.1) Genera tu User Access Token en https://huggingface.co/settings/tokens (scope: \"read\" para descargar; \"write\" si vas a subir)\n",
        "# 4.2) En Colab: usa input seguro\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = getpass(\"HF token (no se mostrará): \")\n",
        "login(token=token)  # almacena el token en ~/.cache/huggingface\n",
        "\n",
        "from huggingface_hub import whoami\n",
        "print(whoami())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7208597e",
      "metadata": {
        "id": "7208597e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579,
          "referenced_widgets": [
            "d8dd37122b19497aa8a4dedb04a982b8",
            "e8867eb0a048424dbf5da35a00447f20",
            "954fe7e4664248d9b4bcdf830efd4869",
            "ff8097c3a0774b3c9ed5c3073d0dbc7a",
            "dcc7e588e09245d5b78d5ed5d4ed6a01",
            "5e0bf3fa95884a6ab2705c79179518eb",
            "5d9fea89a26d49aeb5c934e9109b1656",
            "b9847aae1cbc4f01b56f8480141a0f29",
            "04165d1379f944baa3d0681447c14ec1",
            "2990860b69224ab6a51ac79922dad841",
            "a6f3a86a8bdb427c97403ea838a49c1c"
          ]
        },
        "outputId": "2fd3e5c7-af32-4a43-fa3f-8e0d9ac43142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando carga de Tokenizer y Modelo 4-bit (QLoRA) ---\n",
            "Paso 1: Configurando BitsAndBytesConfig...\n",
            "BitsAndBytesConfig configurado.\n",
            "  BitsAndBytesConfig details: BitsAndBytesConfig {\n",
            "  \"_load_in_4bit\": true,\n",
            "  \"_load_in_8bit\": false,\n",
            "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": false,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": null,\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            "\n",
            "Paso 2: Cargando Tokenizer desde meta-llama/Llama-3.1-8B-Instruct...\n",
            "Tokenizer cargado en 0.68 segundos.\n",
            "Pad token configurado: <|eot_id|>, Pad token ID: 128009\n",
            "  Tokenizer class: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
            "  Tokenizer vocab size: 128000\n",
            "Paso 3: Cargando Modelo 4-bit desde meta-llama/Llama-3.1-8B-Instruct...de Hugging Face\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8dd37122b19497aa8a4dedb04a982b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 4-bit cargado en 24.72 segundos.\n",
            "  Model class: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
            "  Model device: cuda:0\n",
            "--- Proceso de carga completado en 25.40 segundos ---\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 5) Carga Tokenizer y Modelo 4-bit (QLoRA)\n",
        "#    [TRANSFORMER] Aquí se instancia el Transformer Llama 3.x\n",
        "# ============================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"--- Iniciando carga de Tokenizer y Modelo 4-bit (QLoRA) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Paso 1: Configurar BitsAndBytesConfig\n",
        "# Esta configuración especifica cómo se cargará y usará el modelo en 4 bits.\n",
        "print(\"Paso 1: Configurando BitsAndBytesConfig...\")\n",
        "# Input principal: Variables de configuración (LOAD_IN_4BIT, BNB_4BIT_QUANT_TYPE, BNB_4BIT_COMPUTE_DTYPE)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=CFG.LOAD_IN_4BIT,\n",
        "    bnb_4bit_quant_type=CFG.BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, CFG.BNB_4BIT_COMPUTE_DTYPE),\n",
        ")\n",
        "print(\"BitsAndBytesConfig configurado.\")\n",
        "# Output principal: Un objeto BitsAndBytesConfig\n",
        "print(f\"  BitsAndBytesConfig details: {bnb_config}\")\n",
        "\n",
        "\n",
        "# Paso 2: Cargar el Tokenizer\n",
        "# AutoTokenizer.from_pretrained carga el tokenizer asociado al modelo base especificado.\n",
        "# Es responsable de convertir texto en IDs numéricos (tokens) y viceversa.\n",
        "print(f\"Paso 2: Cargando Tokenizer desde {CFG.BASE_MODEL}...\")\n",
        "tokenizer_start_time = time.time()\n",
        "# Input principal: CFG.BASE_MODEL (string con el nombre/ruta del modelo)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    use_fast=True, # Usar la versión rápida del tokenizer (si está disponible)\n",
        "    padding_side=\"right\" # Configurar dónde añadir el padding (importante para modelos causales)\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Asegurar que haya un token de padding\n",
        "tokenizer_end_time = time.time()\n",
        "print(f\"Tokenizer cargado en {tokenizer_end_time - tokenizer_start_time:.2f} segundos.\")\n",
        "print(f\"Pad token configurado: {tokenizer.pad_token}, Pad token ID: {tokenizer.pad_token_id}\")\n",
        "# Output principal: Un objeto Tokenizer\n",
        "print(f\"  Tokenizer class: {type(tokenizer)}\")\n",
        "print(f\"  Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "\n",
        "# Paso 3: Cargar el Modelo en 4-bit\n",
        "# AutoModelForCausalLM.from_pretrained carga el modelo base pre-entrenado.\n",
        "# La cuantización BitsAndBytesConfig se aplica durante este proceso si LOAD_IN_4BIT es True.\n",
        "print(f\"Paso 3: Cargando Modelo 4-bit desde {CFG.BASE_MODEL}...de Hugging Face\")\n",
        "model_start_time = time.time()\n",
        "# Input principal: CFG.BASE_MODEL (string), quantization_config (objeto BitsAndBytesConfig), device_map\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    quantization_config=bnb_config, # Aplicar la configuración de cuantización\n",
        "    device_map=\"auto\", # Distribuir las capas del modelo automáticamente entre los dispositivos disponibles (GPU)\n",
        "    trust_remote_code=False # No ejecutar código arbitrario del Hub (generalmente True por seguridad)\n",
        ")\n",
        "model_end_time = time.time()\n",
        "print(f\"Modelo 4-bit cargado en {model_end_time - model_start_time:.2f} segundos.\")\n",
        "# Output principal: Un objeto AutoModelForCausalLM (cuantizado si se configuró así)\n",
        "print(f\"  Model class: {type(model)}\")\n",
        "print(f\"  Model device: {model.device}\") # Note: device_map=\"auto\" might show base device or a range\n",
        "\n",
        "\n",
        "# Configuraciones adicionales del modelo cargado\n",
        "model.config.use_cache = False # Deshabilitar caché durante el entrenamiento (ahorra memoria, útil para gradient checkpointing)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id # Asegurar que el modelo conozca el ID del token de padding\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Proceso de carga completado en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ae090029",
      "metadata": {
        "id": "ae090029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa62b529-fad0-4140-fe1f-1e1e327df755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando preparación PEFT ---\n",
            "Paso 1: Preparando modelo para entrenamiento k-bit...\n",
            "  Input a prepare_model_for_kbit_training: Model of type <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
            "Modelo preparado para entrenamiento k-bit.\n",
            "Paso 2: Definiendo LoraConfig con los siguientes parámetros:\n",
            "  r: 16\n",
            "  lora_alpha: 32\n",
            "  lora_dropout: 0.05\n",
            "  target_modules: ('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj')\n",
            "  task_type: CAUSAL_LM\n",
            "LoraConfig definido.\n",
            "  Output de LoraConfig: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'v_proj', 'q_proj', 'k_proj', 'gate_proj', 'o_proj', 'down_proj', 'up_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)\n",
            "Paso 3: Inyectando adaptadores LoRA con get_peft_model...\n",
            "  Input 1 a get_peft_model: Model of type <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
            "  Input 2 a get_peft_model: LoraConfig of type <class 'peft.tuners.lora.config.LoraConfig'>\n",
            "Adaptadores LoRA inyectados. Modelo PEFT creado.\n",
            "  Output de get_peft_model: Model of type <class 'peft.peft_model.PeftModelForCausalLM'>\n",
            "\n",
            "--- Resumen de parámetros entrenables ---\n",
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
            "----------------------------------------\n",
            "--- Preparación PEFT completada en 0.68 segundos ---\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 6) Preparación PEFT (LoRA sobre QLoRA)\n",
        "#    [TRANSFORMER] Inyectamos adaptadores LoRA en q/k/v/o y MLP\n",
        "# ============================================================\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import time # Import time for logging\n",
        "\n",
        "print(\"--- Iniciando preparación PEFT ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Paso 1: Preparar el modelo base cargado en 4-bit para entrenamiento k-bit (QLoRA)\n",
        "# Esta función aplica pre-procesamiento al modelo, como habilitar gradient checkpointing\n",
        "# si está configurado, y preparar los embeddings para el entrenamiento en 4-bit.\n",
        "print(\"Paso 1: Preparando modelo para entrenamiento k-bit...\")\n",
        "# Input principal: El objeto 'model' cargado en 4-bit de la celda anterior.\n",
        "print(f\"  Input a prepare_model_for_kbit_training: Model of type {type(model)}\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"Modelo preparado para entrenamiento k-bit.\")\n",
        "# Output principal: El mismo objeto 'model', pero con hooks y configuraciones para k-bit training.\n",
        "\n",
        "\n",
        "# Paso 2: Definir la configuración de LoRA\n",
        "# LoraConfig define los hiperparámetros del adaptador LoRA que se inyectará.\n",
        "# Estos hiperparámetros controlan el tamaño (r, lora_alpha) y la regularización (lora_dropout)\n",
        "# de las matrices de bajo rango, así como a qué módulos del modelo base se aplicarán (target_modules).\n",
        "print(\"Paso 2: Definiendo LoraConfig con los siguientes parámetros:\")\n",
        "print(f\"  r: {CFG.LORA_R}\")\n",
        "print(f\"  lora_alpha: {CFG.LORA_ALPHA}\")\n",
        "print(f\"  lora_dropout: {CFG.LORA_DROPOUT}\")\n",
        "print(f\"  target_modules: {CFG.TARGET_MODULES}\")\n",
        "print(f\"  task_type: {CFG.TASK_TYPE}\")\n",
        "peft_config = LoraConfig(\n",
        "    r=CFG.LORA_R,\n",
        "    lora_alpha=CFG.LORA_ALPHA,\n",
        "    lora_dropout=CFG.LORA_DROPOUT,\n",
        "    target_modules=list(CFG.TARGET_MODULES), # Aseguramos que sea una lista si target_modules es tupla\n",
        "    task_type=CFG.TASK_TYPE,\n",
        "    bias=\"none\" # Generalmente \"none\" para fine-tuning de modelos generativos\n",
        ")\n",
        "print(\"LoraConfig definido.\")\n",
        "# Output principal: Un objeto LoraConfig.\n",
        "print(f\"  Output de LoraConfig: {peft_config}\")\n",
        "\n",
        "\n",
        "# Paso 3: Obtener el modelo PEFT (inyectar los adaptadores LoRA)\n",
        "# Esta función toma el modelo base y la configuración LoRA, y devuelve\n",
        "# un \"PeftModel\", que es el modelo base con los adaptadores LoRA inyectados\n",
        "# y configurado para entrenar solo esos adaptadores.\n",
        "print(\"Paso 3: Inyectando adaptadores LoRA con get_peft_model...\")\n",
        "# Inputs: El modelo preparado para k-bit training y el objeto LoraConfig.\n",
        "print(f\"  Input 1 a get_peft_model: Model of type {type(model)}\")\n",
        "print(f\"  Input 2 a get_peft_model: LoraConfig of type {type(peft_config)}\")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\"Adaptadores LoRA inyectados. Modelo PEFT creado.\")\n",
        "# Output principal: Un objeto PeftModel.\n",
        "print(f\"  Output de get_peft_model: Model of type {type(model)}\")\n",
        "\n",
        "\n",
        "# Mostrar parámetros entrenables\n",
        "# Esto imprime cuántos parámetros totales tiene el modelo base y cuántos\n",
        "# parámetros adicionales (los de LoRA) serán entrenados.\n",
        "print(\"\\n--- Resumen de parámetros entrenables ---\")\n",
        "model.print_trainable_parameters()\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Preparación PEFT completada en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb20f7ab",
      "metadata": {
        "id": "bb20f7ab"
      },
      "source": [
        "\n",
        "> **Alternativas PEFT**: IA3/AdaLoRA (cambia `peft_config`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f2f31f4",
      "metadata": {
        "id": "8f2f31f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2032ebd-3b11-4adf-f10c-bf86dc7e8d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando carga de Dataset ---\n",
            "  Intentando cargar dataset desde: local_path='/content/drive/MyDrive/datasets/arqsoft_chat.jsonl', hf_id='None'\n",
            "  Archivo local encontrado en: /content/drive/MyDrive/datasets/arqsoft_chat.jsonl. Cargando desde JSONL...\n",
            "  Dataset cargado desde JSONL local. Número de ejemplos: 2500\n",
            "\n",
            "--- Primer ejemplo del Dataset cargado (Output de load_chat_dataset) ---\n",
            "{'messages': [{'content': 'Eres un asistente experto en Arquitectura de Software. Respondes con claridad, precisión técnica, cuadros comparativos cuando aplica y ejemplos prácticos orientados a microservicios, EDA/Kafka, API Management, DevOps y seguridad.', 'role': 'system'}, {'content': 'Define una estrategia de Alertas Prometheus (latencia/errores/lag) con herramientas concretas y KPIs.', 'role': 'user'}, {'content': '- Instrumenta con OpenTelemetry (SDK HTTP/Kafka/DB).\\n- Métricas RED/USE y dashboards en Grafana.\\n- Alertas Prometheus: latencia p95/p99, error_rate, lag Kafka.\\n- Trazas con muestreo adaptativo y *exemplar linking*.\\n- Automatiza despliegue con GitOps (ArgoCD) y *progressive delivery*.\\n\\n**KPIs sugeridos**\\n- Disponibilidad ≥ 99.9%, error_rate < 1%.\\n- Tiempo de rollback < 5 min.\\n- MTTR < 15 min; cobertura de trazas > 60%.', 'role': 'assistant'}]}\n",
            "---------------------------------------\n",
            "--- Proceso de carga de Dataset completado en 0.06 segundos ---\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7) Carga de Dataset (formato chat)\n",
        "# ============================================================\n",
        "from datasets import load_dataset, Dataset\n",
        "import json, os\n",
        "import time # Import time for logging\n",
        "\n",
        "print(\"--- Iniciando carga de Dataset ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Define una función para cargar el dataset desde un archivo local JSONL o desde Hugging Face Hub\n",
        "# Prioriza el archivo local si se especifica y existe.\n",
        "def load_chat_dataset(local_path: str | None, hf_id: str | None):\n",
        "    # --- Logs de entrada a la función ---\n",
        "    # Muestra los valores de los inputs local_path y hf_id que recibe la función.\n",
        "    print(f\"  Intentando cargar dataset desde: local_path='{local_path}', hf_id='{hf_id}'\")\n",
        "    # -----------------------------------\n",
        "    if local_path and os.path.exists(local_path):\n",
        "        print(f\"  Archivo local encontrado en: {local_path}. Cargando desde JSONL...\")\n",
        "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            # Lee cada línea como un objeto JSON (asumiendo que cada línea es un ejemplo de chat)\n",
        "            records = [json.loads(line) for line in f]\n",
        "        # Convierte la lista de diccionarios en un objeto Dataset de Hugging Face\n",
        "        dataset = Dataset.from_list(records)\n",
        "        print(f\"  Dataset cargado desde JSONL local. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "    elif hf_id:\n",
        "        print(f\"  Archivo local no encontrado o no especificado. Intentando cargar desde Hugging Face Hub: {hf_id}\")\n",
        "        # Carga el dataset directamente desde Hugging Face Hub\n",
        "        # Assume split 'train' by default, you might need to adjust this based on the dataset structure\n",
        "        dataset = load_dataset(hf_id, split=\"train\")\n",
        "        print(f\"  Dataset cargado desde Hugging Face Hub. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "    else:\n",
        "        print(\"  No se especificó archivo local ni ID de Hugging Face. Usando dataset mini de ejemplo.\")\n",
        "        # Si no se especifica ninguna fuente válida, crea un pequeño dataset de ejemplo\n",
        "        mini = [\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.\"}\n",
        "            ]},\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Diseña un patrón EDA en Kafka para fidelización al 99.99%.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"Particiones y RF≥3, acks=all, min.insync.replicas=2, DLQ, Schema Registry, idempotent producer, outbox, SLO/SLI y alertas por latencia/lag.\"}\n",
        "            ]},\n",
        "        ]\n",
        "        dataset = Dataset.from_list(mini)\n",
        "        print(f\"  Dataset mini de ejemplo creado. Número de ejemplos: {len(dataset)}\")\n",
        "        return dataset\n",
        "\n",
        "# Llama a la función para cargar el dataset utilizando las rutas/IDs de la configuración\n",
        "# Inputs a la función: CFG.DATASET_LOCAL_JSONL y CFG.DATASET_HF_ID definidos en la celda 2.\n",
        "raw_ds = load_chat_dataset(CFG.DATASET_LOCAL_JSONL, CFG.DATASET_HF_ID)\n",
        "\n",
        "# --- Log del primer ejemplo del dataset cargado ---\n",
        "# Muestra la estructura y contenido del primer ejemplo del dataset cargado (output de la función).\n",
        "print(\"\\n--- Primer ejemplo del Dataset cargado (Output de load_chat_dataset) ---\")\n",
        "print(raw_ds[0])\n",
        "print(\"---------------------------------------\")\n",
        "# --------------------------------------------\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Proceso de carga de Dataset completado en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fbf15fb0",
      "metadata": {
        "id": "fbf15fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "62d124982cf9458e912ce6fade94e78f",
            "f14f0400525c41438d5718e83692aace",
            "5179c6c01ac64bf29ed10bd656bb5dbe",
            "dc9e6963419a4544bdd750c32d24082d",
            "4e8848bc771b4411a6e45b69ad949361",
            "1ba186c2c9ff44248f34cf9e4ca3df1c",
            "1e06171722dd4097b18c7f156e98aead",
            "652e7430285245c2809ad73818650516",
            "419b6536d5de4e899b1a2a63fc4512e1",
            "331fe9bc2c174545b89893c4a9b74524",
            "2476dcb497444507962303c6f282ad6a",
            "45f243ae00c54dbab603d47c47fd2cc9",
            "444dde63a9214207877e175ac20953e1",
            "1ece0b210ac547f2a764419c3679e18b",
            "da350c3428f145d895f9c878c9f27056",
            "0451e19361fd4a408c9cccb0f369fcfc",
            "d700ec4e43ae42c7961c036b3197fa7a",
            "ddb6a50172b14f9186d7ebce99754a8c",
            "fb7c68816ed549579d07832ad6515193",
            "ad35f5dce8c7469fa7ba2a3eac5509fb",
            "e0f2db4a54fd4013b1637fb6b3a22127",
            "e2b158b2070442f7b642e4aab759467a"
          ]
        },
        "outputId": "b8cfab56-9fd0-44f5-aaa4-634b2b951e80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62d124982cf9458e912ce6fade94e78f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45f243ae00c54dbab603d47c47fd2cc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset de Entrenamiento: 2000 ejemplos\n",
            "Dataset de Validación: 500 ejemplos\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 8) Transformación de datos\n",
        "#    [DATA TRANSFORM] chat_template → tokenización → labels (pad→-100)\n",
        "# ============================================================\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=CFG.MAX_SEQ_LEN,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    if input_ids and isinstance(input_ids[0], list):\n",
        "        labels = [\n",
        "            [tok if tok != pad_id else -100 for tok in seq]\n",
        "            for seq in input_ids\n",
        "        ]\n",
        "    else:\n",
        "        labels = [tok if tok != pad_id else -100 for tok in input_ids]\n",
        "\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names)\n",
        "\n",
        "# Divide el dataset procesado en conjuntos de entrenamiento y validación\n",
        "split = raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names).train_test_split(test_size=0.20, seed=42)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "# Muestra el número de ejemplos en cada conjunto\n",
        "print(f\"Dataset de Entrenamiento: {len(train_ds)} ejemplos\")\n",
        "print(f\"Dataset de Validación: {len(val_ds)} ejemplos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6681e6e5",
      "metadata": {
        "id": "6681e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "432e511b-f31b-4cf2-de1c-6cfc8d017ca5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Configurando SFTConfig para el entrenamiento ---\n",
            "  Output directory: /content/drive/MyDrive/llama3_arqsoft_peft\n",
            "  Max sequence length: 1024\n",
            "  Per device train batch size: 1\n",
            "  Gradient accumulation steps: 16\n",
            "  Learning rate: 0.0002\n",
            "  Logging steps: 10\n",
            "  Eval strategy: no\n",
            "  Eval steps: 100\n",
            "  Save steps: 200\n",
            "  Use BF16: False\n",
            "  Use FP16: True\n",
            "  Warmup ratio: 0.03\n",
            "  Max steps (calculated): 200\n",
            "  Num train epochs (calculated): 1000\n",
            "  Gradient checkpointing: True\n",
            "SFTConfig configurado.\n",
            "\n",
            "--- Inicializando SFTTrainer ---\n",
            "  Input model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
            "  Input tokenizer type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
            "  Input train_dataset size: 2000 ejemplos\n",
            "  Input eval_dataset size: 500 ejemplos\n",
            "  Input data_collator: <function default_data_collator at 0x7fe2ed806f20>\n",
            "SFTTrainer inicializado.\n",
            "\n",
            "--- Listo para iniciar el entrenamiento ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  9/200 06:46 < 3:04:58, 0.02 it/s, Epoch 0.06/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 3:13:27, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.068500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.028900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.022800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.021700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.021400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.020500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.020400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Entrenamiento finalizado ---\n",
            "\n",
            "--- Guardando adaptador PEFT en /content/drive/MyDrive/llama3_arqsoft_peft/arqsoft-qlora ---\n",
            "--- Adaptador y tokenizer guardados ---\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 9) Entrenamiento (TRL SFTTrainer)\n",
        "# ============================================================\n",
        "try:\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "except ModuleNotFoundError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    print(\"Instalando TRL (trl==0.11.4)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl==0.11.4\"])\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "# Determine eval_strategy based on max_steps\n",
        "# Si MAX_STEPS > 0, la estrategia de evaluación se establece en \"no\" para evitar\n",
        "# evaluaciones intermedias basadas en pasos que podrían no alinearse bien con los checkpoints.\n",
        "# Si MAX_STEPS <= 0 (entrenamiento por épocas), la evaluación se basa en pasos (\"steps\").\n",
        "eval_strategy = \"steps\" if CFG.MAX_STEPS <= 0 else \"no\"\n",
        "\n",
        "# SFTConfig define los hiperparámetros y configuraciones para el entrenamiento de fine-tuning supervisado (SFT).\n",
        "# Es similar a TrainingArguments de la librería transformers, pero adaptada para SFT en TRL.\n",
        "print(\"--- Configurando SFTConfig para el entrenamiento ---\")\n",
        "# Inputs principales: Variables de configuración definidas en la clase Config (celda 2).\n",
        "print(f\"  Output directory: {CFG.OUTPUT_DIR}\")\n",
        "print(f\"  Max sequence length: {CFG.MAX_SEQ_LEN}\")\n",
        "print(f\"  Per device train batch size: {CFG.PER_DEVICE_BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation steps: {CFG.GRADIENT_ACCUMULATION}\")\n",
        "print(f\"  Learning rate: {CFG.LEARNING_RATE}\")\n",
        "print(f\"  Logging steps: {CFG.LOGGING_STEPS}\")\n",
        "print(f\"  Eval strategy: {eval_strategy}\")\n",
        "print(f\"  Eval steps: {CFG.EVAL_STEPS}\")\n",
        "print(f\"  Save steps: {CFG.SAVE_STEPS}\")\n",
        "print(f\"  Use BF16: {CFG.USE_BF16}\")\n",
        "print(f\"  Use FP16: {CFG.USE_FP16}\")\n",
        "print(f\"  Warmup ratio: {CFG.WARMUP_RATIO}\")\n",
        "print(f\"  Max steps (calculated): {CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1}\")\n",
        "print(f\"  Num train epochs (calculated): {CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000}\")\n",
        "print(f\"  Gradient checkpointing: {CFG.GRADIENT_CHECKPOINTING}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    max_seq_length=CFG.MAX_SEQ_LEN,\n",
        "    per_device_train_batch_size=CFG.PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=CFG.GRADIENT_ACCUMULATION,\n",
        "    learning_rate=CFG.LEARNING_RATE,\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    eval_strategy=eval_strategy, # Use the determined strategy\n",
        "    eval_steps=CFG.EVAL_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    bf16=CFG.USE_BF16,\n",
        "    fp16=CFG.USE_FP16,\n",
        "    warmup_ratio=CFG.WARMUP_RATIO,\n",
        "    max_steps=CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1, # Use -1 for no max steps\n",
        "    num_train_epochs=CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000, # Set epochs to a large value if max_steps is used\n",
        "    gradient_checkpointing=CFG.GRADIENT_CHECKPOINTING,\n",
        "    report_to=[\"none\"], # Deshabilita reportes a plataformas como Weights & Biases por defecto\n",
        ")\n",
        "print(\"SFTConfig configurado.\")\n",
        "# Output principal: Un objeto SFTConfig.\n",
        "\n",
        "\n",
        "# SFTTrainer es la clase principal de TRL para realizar el fine-tuning supervisado.\n",
        "# Abstrae el bucle de entrenamiento estándar, la preparación de datos, la optimización,\n",
        "# y la evaluación, especialmente optimizado para modelos grandes y PEFT.\n",
        "print(\"\\n--- Inicializando SFTTrainer ---\")\n",
        "# Inputs principales:\n",
        "# - model: El modelo PEFT preparado (de la celda 6).\n",
        "# - tokenizer: El tokenizer cargado (de la celda 5).\n",
        "# - args: El objeto SFTConfig con las configuraciones de entrenamiento.\n",
        "# - train_dataset: El dataset de entrenamiento procesado (de la celda 8).\n",
        "# - eval_dataset: El dataset de validación procesado (de la celda 8).\n",
        "# - data_collator: Una función para agrupar ejemplos del dataset en batches.\n",
        "print(f\"  Input model type: {type(model)}\")\n",
        "print(f\"  Input tokenizer type: {type(tokenizer)}\")\n",
        "#print(f\"  Input args (SFTConfig): {sft_config}\")\n",
        "print(f\"  Input train_dataset size: {len(train_ds)} ejemplos\")\n",
        "print(f\"  Input eval_dataset size: {len(val_ds)} ejemplos\")\n",
        "print(f\"  Input data_collator: {default_data_collator}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "print(\"SFTTrainer inicializado.\")\n",
        "# Output principal: Un objeto SFTTrainer.\n",
        "\n",
        "\n",
        "# Para iniciar el entrenamiento, descomenta la línea trainer.train()\n",
        "print(\"\\n--- Listo para iniciar el entrenamiento ---\")\n",
        "trainer.train()\n",
        "print(\"--- Entrenamiento finalizado ---\")\n",
        "\n",
        "# Para guardar el adaptador después del entrenamiento, descomenta estas líneas\n",
        "print(f\"\\n--- Guardando adaptador PEFT en {CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME} ---\")\n",
        "trainer.model.save_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\")\n",
        "tokenizer.save_pretrained(CFG.OUTPUT_DIR) # Guarda el tokenizer también\n",
        "print(\"--- Adaptador y tokenizer guardados ---\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "7fc2b2b9e2ca4624be6a8fc37ed5af26",
            "97e501ac279c4711a8605a9b2d2284e5",
            "8a235fa054114942a20471a61e02e45b",
            "5d3523f1cdbe4936b5235e28acfebfb4",
            "6c56580fe9064921b228a4c41ead886d",
            "a52c821e33424d0597fc38315edc546e",
            "349f71bf6f9b45b68d8dfe65c88f4fb7",
            "5a69cf74beee4fc5963993e31386077d",
            "b755e53d12354ae3ad11102875375a82",
            "4cff6690cb3444efbecfcef581cd876e",
            "1ec7586142db4cf2a7f5a3250a26cd86",
            "8dbc5d1b450a4af58d3dfadb609e9189",
            "eee87a707c294adabf3aeec445b559a2",
            "ce26e53fb3754cf5a35b19a562719474",
            "c53bc1ac3984414198e0b5e18b502db9",
            "d3bce5bd0511444c9b9a37a8e677e8a4",
            "26f5e23cd5cc4832bbb6a83dcfef4871",
            "10aa9a0506da4ec4bc783e7c1dc638ea",
            "31d555b5846a40fbbfb8ca416766bbd5",
            "898b59b3ae6544eea9d7434d8c00ec1c",
            "58ec9b75d436432a8362a68eb1c066ba",
            "494ea1f947bd4a00bc37de5766a7c5dd",
            "090506eaa49843bdacc867ba1b1d854c",
            "2a335dad366e4691a276c4cf23ee9ad2",
            "15bcaef13c9d4fe3943b09c1b7d4951f",
            "958066b430be423982f079af8ed17273",
            "31b6021a9c7d4c269f3b52891115f0cb",
            "a00fb28587f34aeab66d04e76beb7a8b",
            "1e21f07e914f481a8b76c27589271d22",
            "fbf39b5a760a4afba464a1fd12bbff6e",
            "e4e28cf072fc4df09ab7ec37cfdb520c",
            "b129c94eaa5e41f6b72a10a595bea1df",
            "09ddce66afd447908bf49f28e27bb400"
          ]
        },
        "id": "d13cf4ce",
        "outputId": "2cbaf667-ebfc-43d2-94de-d1ba2ef29d7c"
      },
      "source": [
        "# ============================================================\n",
        "# 9b) (Opcional) Subir el adaptador PEFT a Hugging Face Hub\n",
        "# ============================================================\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "# Asegúrate de que el adaptador entrenado se haya guardado en la celda 9\n",
        "# Puedes subir el directorio completo que contiene el adapter_config.json y adapter_model.safetensors\n",
        "\n",
        "# Define la ruta local donde se guardó el adaptador\n",
        "adapter_local_path = f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\"\n",
        "\n",
        "# Define el nombre del repositorio en Hugging Face Hub\n",
        "# ¡¡¡REEMPLAZA con tu nombre de usuario y el nombre de tu repositorio!!!\n",
        "hf_repo_id = \"jrosado1974/llama3-arqsoft-adapter\" # Ejemplo: \"mi-usuario/llama3-arqsoft-adapter\"\n",
        "\n",
        "print(f\"--- Intentando subir el adaptador desde {adapter_local_path} a Hugging Face Hub ({hf_repo_id}) ---\")\n",
        "\n",
        "# Crea una instancia de HfApi (se autenticará usando el token de la celda 4)\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    # Sube el directorio completo del adaptador\n",
        "    # Asegúrate de que el repositorio 'hf_repo_id' ya exista en tu cuenta de Hugging Face\n",
        "    api.upload_folder(\n",
        "        folder_path=adapter_local_path,\n",
        "        repo_id=hf_repo_id,\n",
        "        repo_type=\"model\", # Súbelo como un modelo\n",
        "        commit_message=f\"Subir adaptador PEFT {CFG.ADAPTER_NAME} entrenado\"\n",
        "    )\n",
        "    print(f\"--- Adaptador subido exitosamente a https://huggingface.co/{hf_repo_id} ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"--- Error al subir el adaptador a Hugging Face Hub: {e} ---\")\n",
        "    print(\"Asegúrate de que:\")\n",
        "    print(\"1. Has ejecutado la celda 4 con un token de escritura ('write').\")\n",
        "    print(\"2. Has ejecutado la celda 9 (y descomentado las líneas de guardado) para crear el adaptador en Drive.\")\n",
        "    print(f\"3. El directorio local del adaptador existe: {adapter_local_path}\")\n",
        "    print(f\"4. Has reemplazado 'tu-nombre-de-usuario/nombre-del-repositorio-adaptador' con tu ID de repositorio correcto.\")\n",
        "    print(\"5. El repositorio 'hf_repo_id' ya existe en tu cuenta de Hugging Face Hub.\")"
      ],
      "id": "d13cf4ce",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Intentando subir el adaptador desde /content/drive/MyDrive/llama3_arqsoft_peft/arqsoft-qlora a Hugging Face Hub (jrosado1974/llama3-arqsoft-adapter) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fc2b2b9e2ca4624be6a8fc37ed5af26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dbc5d1b450a4af58d3dfadb609e9189"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...adapter_model.safetensors:   0%|          |  556kB /  168MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "090506eaa49843bdacc867ba1b1d854c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Adaptador subido exitosamente a https://huggingface.co/jrosado1974/llama3-arqsoft-adapter ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f53c70f7",
      "metadata": {
        "id": "f53c70f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87f8e43-99a4-4ec0-dbd1-716bdb88f778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Detalles de Inferencia ---\n",
            "Modelo base: meta-llama/Llama-3.1-8B-Instruct\n",
            "Usando PEFT/LoRA: True\n",
            "Dispositivo del modelo: cuda:0\n",
            "Texto de entrada tokenizado: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "Eres un asistente experto en Arquitectura de Software.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Indicame en que se diferencia spring boot de quarkus<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "...\n",
            "Longitud del texto de entrada: 339\n",
            "Inputs tensor shape: torch.Size([1, 65])\n",
            "Inputs tensor device: cuda:0\n",
            "Attention mask shape: torch.Size([1, 65])\n",
            "Attention mask device: cuda:0\n",
            "Max new tokens: 256\n",
            "Temperature: 0.2\n",
            "Top P: 0.95\n",
            "-----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "Eres un asistente experto en Arquitectura de Software.user\n",
            "\n",
            "Indicame en que se diferencia spring boot de quarkusassistant\n",
            "\n",
            "- **Ámbito**: Spring está más enfocado en microservicios, EDA/Kafka, API Management, seguridad y escala vertical. Quarkus se centra en monolitos/modulos, BFF, API Composition y costos.\n",
            "- **Funciones**: Spring ofrece herramientas concretas para cada área (RDBMS, NoSQL, API Management, seguridad), mientras que Quarkus proporciona herramientas generales para microservicios y EDA.\n",
            "- **Ecosistema**: Spring tiene un ecosistema más maduro y amplio, con más herramientas y herramientas concretas. Quarkus cuenta con herramientas mínimas pero de alta calidad y compatibilidad con standards.\n",
            "\n",
            "En resumen, si estás building monolitos pequeños, usa Quarkus. Si estás building una infraestructura compleja, usa Spring.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 10) Inferencia de prueba\n",
        "# ============================================================\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "def chat(prompt: str, sys: str = \"Eres un asistente experto en Arquitectura de Software.\"):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\": sys},\n",
        "        {\"role\":\"user\",\"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # --- Agregar logs aquí ---\n",
        "    print(\"\\n--- Detalles de Inferencia ---\")\n",
        "    print(f\"Modelo base: {CFG.BASE_MODEL}\")\n",
        "    print(f\"Usando PEFT/LoRA: {isinstance(model, PeftModel)}\")\n",
        "    print(f\"Dispositivo del modelo: {model.device}\")\n",
        "    print(f\"Texto de entrada tokenizado: {text[:500]}...\") # Imprime los primeros 500 caracteres\n",
        "    print(f\"Longitud del texto de entrada: {len(text)}\")\n",
        "    # --------------------------\n",
        "\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # --- Más logs sobre los inputs ---\n",
        "    print(f\"Inputs tensor shape: {inputs['input_ids'].shape}\")\n",
        "    print(f\"Inputs tensor device: {inputs['input_ids'].device}\")\n",
        "    print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
        "    print(f\"Attention mask device: {inputs['attention_mask'].device}\")\n",
        "    print(f\"Max new tokens: {CFG.MAX_NEW_TOKENS}\")\n",
        "    print(f\"Temperature: {CFG.TEMPERATURE}\")\n",
        "    print(f\"Top P: {CFG.TOP_P}\")\n",
        "    print(\"-----------------------------\")\n",
        "    # -------------------------------\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
        "            temperature=CFG.TEMPERATURE,\n",
        "            top_p=CFG.TOP_P,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(chat(\"Indicame en que se diferencia spring boot de quarkus\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faeecd54",
      "metadata": {
        "id": "faeecd54"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11) (Opcional) Merge del adaptador y exportación\n",
        "# ============================================================\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import os\n",
        "import torch # Import torch\n",
        "\n",
        "# Define an offload directory (still needed if device_map is not used but model is large)\n",
        "offload_directory = \"/tmp/offload\"\n",
        "os.makedirs(offload_directory, exist_ok=True)\n",
        "\n",
        "# Determine device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "merged = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\",\n",
        "    # device_map=\"auto\", # Remove auto device mapping\n",
        "    # Use explicit device if needed, or rely on default\n",
        "    offload_folder=offload_directory # Keep offload directory as a fallback/option\n",
        ").to(device) # Explicitly move to device\n",
        "\n",
        "merged = merged.merge_and_unload()\n",
        "merged.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a431da",
      "metadata": {
        "id": "c3a431da"
      },
      "source": [
        "\n",
        "## Troubleshooting (T4 + CUDA 12.6)\n",
        "- **bnb sin GPU / no lib cuda** → Repite la celda **0** y luego reinicia runtime. Verifica en **1)** que aparezca `libbitsandbytes_cuda126.so`.\n",
        "- **`bfloat16` no soportado** → Ya configurado (`USE_BF16=False`, `USE_FP16=True`).\n",
        "- **OOM** → Baja `MAX_SEQ_LEN` (1024→768/512), deja `BATCH=1`, mantén `GRADIENT_ACCUMULATION` alto, `gradient_checkpointing=True`.\n",
        "- **labels/pad** → Función de tokenización convierte PAD→`-100`.\n",
        "- **flash-attn/xformers** → Opcionales; SDPA de PyTorch es suficiente en T4.\n",
        "\n",
        "### Resumen didáctico\n",
        "- **[TRANSFORMER]**: celda **5** instancia `AutoModelForCausalLM` (Llama 3.x); **celda 6** inyecta LoRA en `q/k/v/o` y MLP.  \n",
        "- **[DATA TRANSFORM]**: celda **8** aplica `apply_chat_template` → `tokenizer` (trunc/pad) → `labels` (pad = -100).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8dd37122b19497aa8a4dedb04a982b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8867eb0a048424dbf5da35a00447f20",
              "IPY_MODEL_954fe7e4664248d9b4bcdf830efd4869",
              "IPY_MODEL_ff8097c3a0774b3c9ed5c3073d0dbc7a"
            ],
            "layout": "IPY_MODEL_dcc7e588e09245d5b78d5ed5d4ed6a01"
          }
        },
        "e8867eb0a048424dbf5da35a00447f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e0bf3fa95884a6ab2705c79179518eb",
            "placeholder": "​",
            "style": "IPY_MODEL_5d9fea89a26d49aeb5c934e9109b1656",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "954fe7e4664248d9b4bcdf830efd4869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9847aae1cbc4f01b56f8480141a0f29",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04165d1379f944baa3d0681447c14ec1",
            "value": 4
          }
        },
        "ff8097c3a0774b3c9ed5c3073d0dbc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2990860b69224ab6a51ac79922dad841",
            "placeholder": "​",
            "style": "IPY_MODEL_a6f3a86a8bdb427c97403ea838a49c1c",
            "value": " 4/4 [00:23&lt;00:00,  4.28s/it]"
          }
        },
        "dcc7e588e09245d5b78d5ed5d4ed6a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0bf3fa95884a6ab2705c79179518eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9fea89a26d49aeb5c934e9109b1656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9847aae1cbc4f01b56f8480141a0f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04165d1379f944baa3d0681447c14ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2990860b69224ab6a51ac79922dad841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f3a86a8bdb427c97403ea838a49c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62d124982cf9458e912ce6fade94e78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f14f0400525c41438d5718e83692aace",
              "IPY_MODEL_5179c6c01ac64bf29ed10bd656bb5dbe",
              "IPY_MODEL_dc9e6963419a4544bdd750c32d24082d"
            ],
            "layout": "IPY_MODEL_4e8848bc771b4411a6e45b69ad949361"
          }
        },
        "f14f0400525c41438d5718e83692aace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba186c2c9ff44248f34cf9e4ca3df1c",
            "placeholder": "​",
            "style": "IPY_MODEL_1e06171722dd4097b18c7f156e98aead",
            "value": "Map: 100%"
          }
        },
        "5179c6c01ac64bf29ed10bd656bb5dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652e7430285245c2809ad73818650516",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_419b6536d5de4e899b1a2a63fc4512e1",
            "value": 2500
          }
        },
        "dc9e6963419a4544bdd750c32d24082d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331fe9bc2c174545b89893c4a9b74524",
            "placeholder": "​",
            "style": "IPY_MODEL_2476dcb497444507962303c6f282ad6a",
            "value": " 2500/2500 [00:03&lt;00:00, 529.04 examples/s]"
          }
        },
        "4e8848bc771b4411a6e45b69ad949361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ba186c2c9ff44248f34cf9e4ca3df1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e06171722dd4097b18c7f156e98aead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "652e7430285245c2809ad73818650516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "419b6536d5de4e899b1a2a63fc4512e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "331fe9bc2c174545b89893c4a9b74524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2476dcb497444507962303c6f282ad6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45f243ae00c54dbab603d47c47fd2cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_444dde63a9214207877e175ac20953e1",
              "IPY_MODEL_1ece0b210ac547f2a764419c3679e18b",
              "IPY_MODEL_da350c3428f145d895f9c878c9f27056"
            ],
            "layout": "IPY_MODEL_0451e19361fd4a408c9cccb0f369fcfc"
          }
        },
        "444dde63a9214207877e175ac20953e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d700ec4e43ae42c7961c036b3197fa7a",
            "placeholder": "​",
            "style": "IPY_MODEL_ddb6a50172b14f9186d7ebce99754a8c",
            "value": "Map: 100%"
          }
        },
        "1ece0b210ac547f2a764419c3679e18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7c68816ed549579d07832ad6515193",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad35f5dce8c7469fa7ba2a3eac5509fb",
            "value": 2500
          }
        },
        "da350c3428f145d895f9c878c9f27056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0f2db4a54fd4013b1637fb6b3a22127",
            "placeholder": "​",
            "style": "IPY_MODEL_e2b158b2070442f7b642e4aab759467a",
            "value": " 2500/2500 [00:03&lt;00:00, 530.01 examples/s]"
          }
        },
        "0451e19361fd4a408c9cccb0f369fcfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d700ec4e43ae42c7961c036b3197fa7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddb6a50172b14f9186d7ebce99754a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7c68816ed549579d07832ad6515193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad35f5dce8c7469fa7ba2a3eac5509fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0f2db4a54fd4013b1637fb6b3a22127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b158b2070442f7b642e4aab759467a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fc2b2b9e2ca4624be6a8fc37ed5af26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97e501ac279c4711a8605a9b2d2284e5",
              "IPY_MODEL_8a235fa054114942a20471a61e02e45b",
              "IPY_MODEL_5d3523f1cdbe4936b5235e28acfebfb4"
            ],
            "layout": "IPY_MODEL_6c56580fe9064921b228a4c41ead886d"
          }
        },
        "97e501ac279c4711a8605a9b2d2284e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a52c821e33424d0597fc38315edc546e",
            "placeholder": "​",
            "style": "IPY_MODEL_349f71bf6f9b45b68d8dfe65c88f4fb7",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "8a235fa054114942a20471a61e02e45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a69cf74beee4fc5963993e31386077d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b755e53d12354ae3ad11102875375a82",
            "value": 1
          }
        },
        "5d3523f1cdbe4936b5235e28acfebfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cff6690cb3444efbecfcef581cd876e",
            "placeholder": "​",
            "style": "IPY_MODEL_1ec7586142db4cf2a7f5a3250a26cd86",
            "value": "  168MB /  168MB, 33.6MB/s  "
          }
        },
        "6c56580fe9064921b228a4c41ead886d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a52c821e33424d0597fc38315edc546e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349f71bf6f9b45b68d8dfe65c88f4fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a69cf74beee4fc5963993e31386077d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b755e53d12354ae3ad11102875375a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4cff6690cb3444efbecfcef581cd876e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec7586142db4cf2a7f5a3250a26cd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dbc5d1b450a4af58d3dfadb609e9189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eee87a707c294adabf3aeec445b559a2",
              "IPY_MODEL_ce26e53fb3754cf5a35b19a562719474",
              "IPY_MODEL_c53bc1ac3984414198e0b5e18b502db9"
            ],
            "layout": "IPY_MODEL_d3bce5bd0511444c9b9a37a8e677e8a4"
          }
        },
        "eee87a707c294adabf3aeec445b559a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f5e23cd5cc4832bbb6a83dcfef4871",
            "placeholder": "​",
            "style": "IPY_MODEL_10aa9a0506da4ec4bc783e7c1dc638ea",
            "value": "New Data Upload               : 100%"
          }
        },
        "ce26e53fb3754cf5a35b19a562719474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d555b5846a40fbbfb8ca416766bbd5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_898b59b3ae6544eea9d7434d8c00ec1c",
            "value": 1
          }
        },
        "c53bc1ac3984414198e0b5e18b502db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ec9b75d436432a8362a68eb1c066ba",
            "placeholder": "​",
            "style": "IPY_MODEL_494ea1f947bd4a00bc37de5766a7c5dd",
            "value": "  168MB /  168MB, 33.6MB/s  "
          }
        },
        "d3bce5bd0511444c9b9a37a8e677e8a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f5e23cd5cc4832bbb6a83dcfef4871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10aa9a0506da4ec4bc783e7c1dc638ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31d555b5846a40fbbfb8ca416766bbd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "898b59b3ae6544eea9d7434d8c00ec1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58ec9b75d436432a8362a68eb1c066ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494ea1f947bd4a00bc37de5766a7c5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "090506eaa49843bdacc867ba1b1d854c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a335dad366e4691a276c4cf23ee9ad2",
              "IPY_MODEL_15bcaef13c9d4fe3943b09c1b7d4951f",
              "IPY_MODEL_958066b430be423982f079af8ed17273"
            ],
            "layout": "IPY_MODEL_31b6021a9c7d4c269f3b52891115f0cb"
          }
        },
        "2a335dad366e4691a276c4cf23ee9ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a00fb28587f34aeab66d04e76beb7a8b",
            "placeholder": "​",
            "style": "IPY_MODEL_1e21f07e914f481a8b76c27589271d22",
            "value": "  ...adapter_model.safetensors: 100%"
          }
        },
        "15bcaef13c9d4fe3943b09c1b7d4951f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbf39b5a760a4afba464a1fd12bbff6e",
            "max": 167832240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4e28cf072fc4df09ab7ec37cfdb520c",
            "value": 167832240
          }
        },
        "958066b430be423982f079af8ed17273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b129c94eaa5e41f6b72a10a595bea1df",
            "placeholder": "​",
            "style": "IPY_MODEL_09ddce66afd447908bf49f28e27bb400",
            "value": "  168MB /  168MB            "
          }
        },
        "31b6021a9c7d4c269f3b52891115f0cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00fb28587f34aeab66d04e76beb7a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e21f07e914f481a8b76c27589271d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbf39b5a760a4afba464a1fd12bbff6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e28cf072fc4df09ab7ec37cfdb520c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b129c94eaa5e41f6b72a10a595bea1df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ddce66afd447908bf49f28e27bb400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}