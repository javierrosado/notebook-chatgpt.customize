{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4eeb8158",
      "metadata": {
        "id": "4eeb8158"
      },
      "source": [
        "\n",
        "# PEFT / QLoRA **(Colab · Python 3 · GPU T4)** — Llama 3.x Instruct · v2\n",
        "\n",
        "Notebook actualizado para **Colab con CUDA 12.6**: incluye correcciones de instalación para **bitsandbytes** (rueda con soporte CUDA actual) y **Triton**, y mantiene ajustes de memoria/precisión para **T4 (16 GB)**.\n",
        "\n",
        "**Objetivo**: adaptar un modelo **Llama 3.x Instruct** a un **ChatGPT especializado en Arquitectura de Software** mediante **PEFT (LoRA/IA3/AdaLoRA)** y **QLoRA**.\n",
        "\n",
        "> Marcadores pedagógicos: **[TRANSFORMER]** indica dónde se usa la arquitectura Transformer. **[DATA TRANSFORM]** indica operaciones de transformación de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df8ee1c2",
      "metadata": {
        "tags": [
          "install"
        ],
        "id": "df8ee1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595b9a57-50d6-458b-acc6-5e1b83801a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: bitsandbytes 0.48.2\n",
            "Uninstalling bitsandbytes-0.48.2:\n",
            "  Successfully uninstalled bitsandbytes-0.48.2\n",
            "Requirement already satisfied: transformers==4.45.2 in /usr/local/lib/python3.12/dist-packages (4.45.2)\n",
            "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.12/dist-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.6.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.2) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.3)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Using cached bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=3.0.0) (75.2.0)\n",
            "Collecting xformers\n",
            "  Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.3)\n",
            "Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.32.post2\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0) Instalación robusta para Colab (CUDA 12.6 / T4) — EJECUTA PRIMERO\n",
        "# ============================================================\n",
        "# Limpieza de paquetes opcionales que suelen causar conflictos y bnb previo\n",
        "!pip uninstall -y flash-attn xformers bitsandbytes || true\n",
        "\n",
        "# Pila base fijada (estable) para evitar regresiones en Colab\n",
        "!pip install -U \"transformers==4.45.2\" \"accelerate==0.34.2\" #   \"datasets==2.20.0\" \"peft==0.13.2\" \"trl==0.11.4\" \"sentencepiece==0.2.0\"\n",
        "\n",
        "# bitsandbytes con binarios recientes (incluye CUDA 12.x)\n",
        "!pip install -U --pre bitsandbytes\n",
        "\n",
        "# Triton requerido por kernels/integraciones (alineado con PyTorch 2.5.x en Colab)\n",
        "!pip install \"triton>=3.0.0\"\n",
        "\n",
        "# (Opcional) Si quieres volver a instalar xformers:\n",
        "!pip install xformers\n",
        "# (Opcional) flash-attn suele ser innecesario en T4, pero si insistes:\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c8b96ea",
      "metadata": {
        "tags": [
          "diagnostic"
        ],
        "id": "7c8b96ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb6b1cb-0f91-4123-97d8-6b329c8b5cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126 | CUDA: 12.6 | CUDA available: True\n",
            "GPU: Tesla T4\n",
            "bitsandbytes: 0.48.2\n",
            "BNB libs: ['/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda130.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda129.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda126.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda125.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda128.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so', '/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda124.so']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 1) Verificación de entorno + bitsandbytes (CUDA 12.6)\n",
        "# ============================================================\n",
        "import torch, platform, sys, os, glob\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n",
        "    bnblibs = glob.glob(os.path.join(os.path.dirname(bnb.__file__), \"libbitsandbytes_cuda*.so\"))\n",
        "    print(\"BNB libs:\", bnblibs)\n",
        "    if not bnblibs:\n",
        "        print(\"⚠️  No se encontraron binarios CUDA de bitsandbytes. Considera reiniciar runtime y re-ejecutar la celda 0.\")\n",
        "except Exception as e:\n",
        "    print(\"bitsandbytes import error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "501ba0a3",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "501ba0a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00c4c63-fc32-4381-db3f-83e432c9060a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(BASE_MODEL='meta-llama/Llama-3.1-8B-Instruct', DATASET_LOCAL_JSONL='/content/drive/MyDrive/datasets/arqsoft_chat.jsonl', DATASET_HF_ID=None, OUTPUT_DIR='/content/drive/MyDrive/outputs/llama3_arqsoft_peft', ADAPTER_NAME='arqsoft-qlora', MAX_STEPS=500, NUM_EPOCHS=1, LEARNING_RATE=0.0002, PER_DEVICE_BATCH_SIZE=1, GRADIENT_ACCUMULATION=16, MAX_SEQ_LEN=1024, WARMUP_RATIO=0.03, LOGGING_STEPS=10, EVAL_STEPS=100, SAVE_STEPS=200, USE_BF16=False, USE_FP16=True, BNB_4BIT_COMPUTE_DTYPE='float16', LOAD_IN_4BIT=True, BNB_4BIT_QUANT_TYPE='nf4', GRADIENT_CHECKPOINTING=True, LORA_R=16, LORA_ALPHA=32, LORA_DROPOUT=0.05, TARGET_MODULES=('q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'), TASK_TYPE='CAUSAL_LM', MAX_NEW_TOKENS=256, TEMPERATURE=0.2, TOP_P=0.95)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2) Configuración global (optimizada para T4 · 16 GB)\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Ruta raíz para guardar/cargar archivos en Google Drive\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "\n",
        "    # Modelo base a usar de Hugging Face Hub\n",
        "    # Valores posibles: Cualquier modelo compatible con AutoModelForCausalLM (ej: \"meta-llama/Llama-3-8B-Instruct\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "    BASE_MODEL: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "    # Ruta local al archivo JSONL con el dataset de chat\n",
        "    DATASET_LOCAL_JSONL: str = f\"{DRIVE_ROOT}/datasets/arqsoft_chat.jsonl\"\n",
        "\n",
        "    # ID del dataset en Hugging Face Hub (si se usa en lugar de local)\n",
        "    # Ejemplo: de ajibawa-2023/Software-Architecture, https://huggingface.co/datasets/ajibawa-2023/Software-Architecture\n",
        "    DATASET_HF_ID: str | None = None\n",
        "\n",
        "    # Directorio de salida para guardar checkpoints (punto de control del modelo) y el adaptador entrenado\n",
        "    OUTPUT_DIR: str = f\"{DRIVE_ROOT}/outputs/llama3_arqsoft_peft\"\n",
        "\n",
        "    # Nombre del adaptador PEFT (usado para guardar/cargar)\n",
        "    ADAPTER_NAME: str = \"arqsoft-qlora\"\n",
        "\n",
        "    # Número máximo de pasos de entrenamiento (iteraciones del optimizador)\n",
        "    # Valores posibles: Un entero positivo. -1 para entrenar por número de épocas.\n",
        "    MAX_STEPS: int = 500\n",
        "\n",
        "    # Número de épocas completas sobre el dataset de entrenamiento, el MAX_STEPS>1, esta variable no se toma en cuenta.\n",
        "    NUM_EPOCHS: int = 1\n",
        "\n",
        "    # Tasa de aprendizaje para el optimizador\n",
        "    # Valores posibles: Un flotante positivo pequeño (ej: 1e-5 a 5e-4)\n",
        "    LEARNING_RATE: float = 2e-4\n",
        "\n",
        "    # Tamaño del batch por dispositivo (GPU)\n",
        "    # Valores posibles: Un entero positivo (usualmente 1 para QLoRA en T4 para ahorrar memoria)\n",
        "    PER_DEVICE_BATCH_SIZE: int = 1\n",
        "\n",
        "    # Número de pasos de acumulación de gradiente\n",
        "    # Valores posibles: Un entero positivo. Batch efectivo = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION\n",
        "    GRADIENT_ACCUMULATION: int = 16\n",
        "\n",
        "    # Longitud máxima de secuencia para tokenizar (padding/truncation)\n",
        "    # Valores posibles: Un entero positivo. Depende del modelo y dataset (ej: 512, 1024, 2048)\n",
        "    MAX_SEQ_LEN: int = 1024\n",
        "\n",
        "    # Proporción de pasos usados para calentamiento (warmup) del learning rate\n",
        "    WARMUP_RATIO: float = 0.03\n",
        "\n",
        "    # Frecuencia para registrar métricas de entrenamiento (en pasos)\n",
        "    LOGGING_STEPS: int = 10\n",
        "\n",
        "    # Frecuencia para evaluar el modelo en el dataset de validación (en pasos)\n",
        "    # Valores posibles: Un entero positivo. Ignorado si eval_strategy=\"no\".\n",
        "    EVAL_STEPS: int = 100\n",
        "\n",
        "    # Frecuencia para guardar checkpoints del modelo (en pasos)\n",
        "    SAVE_STEPS: int = 200\n",
        "\n",
        "    # Usar precisión bfloat16 (requiere GPU Ampere+ y soporte)\n",
        "    # Valores posibles: bool (True/False). False para T4.\n",
        "    USE_BF16: bool = False\n",
        "\n",
        "    # Usar precisión float16 (más amplio soporte en GPUs como T4)\n",
        "    # Valores posibles: bool (True/False). True para T4.\n",
        "    USE_FP16: bool = True\n",
        "\n",
        "    # Tipo de dato para la computación en 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"float16\", \"bfloat16\" (si la GPU lo soporta)\n",
        "    BNB_4BIT_COMPUTE_DTYPE: str = \"float16\"\n",
        "\n",
        "    # Cargar el modelo en 4 bits usando bitsandbytes\n",
        "    LOAD_IN_4BIT: bool = True\n",
        "\n",
        "    # Tipo de cuantización de 4 bits (bitsandbytes)\n",
        "    # Valores posibles: \"nf4\", \"fp4\"\n",
        "    BNB_4BIT_QUANT_TYPE: str = \"nf4\"\n",
        "\n",
        "    # Habilitar gradient checkpointing para ahorrar memoria\n",
        "    # Valores posibles: bool (True/False). Recomendado en T4.\n",
        "    GRADIENT_CHECKPOINTING: bool = True\n",
        "\n",
        "    # Parámetro 'r' para LoRA: dimensión de los adaptadores\n",
        "    # Valores posibles: Un entero positivo (ej: 8, 16, 32, 64)\n",
        "    LORA_R: int = 16\n",
        "\n",
        "    # Parámetro 'lora_alpha' para LoRA: factor de escalado\n",
        "    # Valores posibles: Un entero positivo (ej: 16, 32, 64). Usualmente >= LORA_R.\n",
        "    LORA_ALPHA: int = 32\n",
        "\n",
        "    # Parámetro 'lora_dropout' para LoRA: dropout en los adaptadores\n",
        "    # Valores posibles: Un flotante entre 0.0 y 1.0\n",
        "    LORA_DROPOUT: float = 0.05\n",
        "\n",
        "    # Módulos del modelo base a los que se aplicará LoRA\n",
        "    TARGET_MODULES: tuple[str, ...] = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
        "\n",
        "    # Tipo de tarea para PEFT (Causal Language Modeling para generación de texto)\n",
        "    # Valores posibles: \"CAUSAL_LM\", \"SEQ_CLS\", etc.\n",
        "    TASK_TYPE: str = \"CAUSAL_LM\"\n",
        "\n",
        "    # Número máximo de tokens a generar durante la inferencia de prueba\n",
        "    MAX_NEW_TOKENS: int = 256\n",
        "\n",
        "    # Temperatura para el muestreo durante la generación (controla la aleatoriedad)\n",
        "    TEMPERATURE: float = 0.2\n",
        "\n",
        "    # Parámetro Top-P para el muestreo durante la generación (controla la diversidad)\n",
        "    TOP_P: float = 0.95\n",
        "\n",
        "CFG = Config()\n",
        "CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5c495ef",
      "metadata": {
        "id": "e5c495ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67960730-e605-41e7-912a-d2855dd79d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 3) (Opcional) Monta Google Drive si tus datos están allí\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Opcional: define carpetas “canónicas”\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "HF_ROOT = f\"{DRIVE_ROOT}/hf_cache\"\n",
        "DATA_ROOT = f\"{DRIVE_ROOT}/datasets\"\n",
        "OUT_ROOT = f\"{DRIVE_ROOT}/llama3_arqsoft_peft\"\n",
        "\n",
        "import os\n",
        "os.makedirs(HF_ROOT, exist_ok=True)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# Redirige caches de Hugging Face (modelos/datasets) a Drive\n",
        "os.environ[\"HF_HOME\"] = HF_ROOT        # raíz HF (recomendado)\n",
        "os.environ[\"HF_HUB_CACHE\"] = f\"{HF_ROOT}/hub\"   # opcional fino\n",
        "\n",
        "# Ajusta tu Config del notebook:\n",
        "CFG.DATASET_LOCAL_JSONL = f\"{DATA_ROOT}/arqsoft_chat.jsonl\"\n",
        "CFG.OUTPUT_DIR = OUT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Ruta del dataset configurada: {CFG.DATASET_LOCAL_JSONL}\")\n",
        "if os.path.exists(CFG.DATASET_LOCAL_JSONL):\n",
        "    print(f\"Archivo encontrado. Tamaño: {os.path.getsize(CFG.DATASET_LOCAL_JSONL)} bytes\")\n",
        "else:\n",
        "    print(\"Archivo NO encontrado en la ruta configurada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLjt6gG4FVVw",
        "outputId": "da18367b-f9ac-4e8d-ec79-248f84e4414d"
      },
      "id": "jLjt6gG4FVVw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta del dataset configurada: /content/drive/MyDrive/datasets/arqsoft_chat.jsonl\n",
            "Archivo encontrado. Tamaño: 2554275 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98c135ea",
      "metadata": {
        "id": "98c135ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d75b751e-c851-48e8-e552-551e3322f7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF token (no se mostrará): ··········\n",
            "{'type': 'user', 'id': '6850d845b8db7f9a70d8bc79', 'name': 'jrosado1974', 'fullname': 'Javier Rosado', 'email': 'javier.rosado@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SX44udlzqpt_Sw7i_nfo7.png', 'orgs': [{'type': 'org', 'id': '66a4f3f3f496b42dc0dd174c', 'name': 'LatinAI', 'fullname': 'AI Developers from Latin America', 'email': None, 'canPay': False, 'periodEnd': None, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65665c2af450504854d60806/l6qHJbnizngi_fnojAI2t.png', 'roleInOrg': 'contributor', 'isEnterprise': False}], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'maestria-uni', 'role': 'write', 'createdAt': '2025-11-06T01:36:23.776Z'}}}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 4) Login a Hugging Face (necesario para descargar Llama 3.x)\n",
        "# ============================================================\n",
        "# 4.1) Genera tu User Access Token en https://huggingface.co/settings/tokens (scope: \"read\" para descargar; \"write\" si vas a subir)\n",
        "# 4.2) En Colab: usa input seguro\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = getpass(\"HF token (no se mostrará): \")\n",
        "login(token=token)  # almacena el token en ~/.cache/huggingface\n",
        "\n",
        "from huggingface_hub import whoami\n",
        "print(whoami())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7208597e",
      "metadata": {
        "id": "7208597e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "550cd272d56f4b23bbb2d2bd8d6e78ac",
            "6be13334b5b546268f6e3d00003b5e1e",
            "5c38c7fd3a5c4b4b99a4ce8a84fcc1fa",
            "5393acb40ea94b9d84942f49128707bf",
            "f38c0e15f38742e2a03f4ad7b5f2785f",
            "5c2d818b378d46188bf2b98e03e54755",
            "e64787c257124978b939fceff401595e",
            "ce3714788d2445cbaca5c65ef52180f1",
            "04dcff6a015646cf8464e449d9140fb3",
            "64f9f233a3d2483f9bd2f3f7077d4d50",
            "60ecae81ce4549a2bdc40287afdf4bb3"
          ]
        },
        "outputId": "edaf6946-5b3f-4b8f-a457-f6fabd301cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando carga de Tokenizer y Modelo 4-bit (QLoRA) ---\n",
            "Paso 1: Configurando BitsAndBytesConfig...\n",
            "BitsAndBytesConfig configurado.\n",
            "Paso 2: Cargando Tokenizer desde meta-llama/Llama-3.1-8B-Instruct...\n",
            "Tokenizer cargado en 0.87 segundos.\n",
            "Pad token configurado: <|eot_id|>, Pad token ID: 128009\n",
            "Paso 3: Cargando Modelo 4-bit desde meta-llama/Llama-3.1-8B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "550cd272d56f4b23bbb2d2bd8d6e78ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 4-bit cargado en 25.17 segundos.\n",
            "--- Proceso de carga completado en 26.04 segundos ---\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 5) Carga Tokenizer y Modelo 4-bit (QLoRA)\n",
        "#    [TRANSFORMER] Aquí se instancia el Transformer Llama 3.x\n",
        "# ============================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"--- Iniciando carga de Tokenizer y Modelo 4-bit (QLoRA) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Paso 1: Configurar BitsAndBytesConfig\n",
        "print(\"Paso 1: Configurando BitsAndBytesConfig...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=CFG.LOAD_IN_4BIT,\n",
        "    bnb_4bit_quant_type=CFG.BNB_4BIT_QUANT_TYPE,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, CFG.BNB_4BIT_COMPUTE_DTYPE),\n",
        ")\n",
        "print(\"BitsAndBytesConfig configurado.\")\n",
        "\n",
        "# Paso 2: Cargar el Tokenizer\n",
        "print(f\"Paso 2: Cargando Tokenizer desde {CFG.BASE_MODEL}...\")\n",
        "tokenizer_start_time = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    use_fast=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer_end_time = time.time()\n",
        "print(f\"Tokenizer cargado en {tokenizer_end_time - tokenizer_start_time:.2f} segundos.\")\n",
        "print(f\"Pad token configurado: {tokenizer.pad_token}, Pad token ID: {tokenizer.pad_token_id}\")\n",
        "\n",
        "\n",
        "# Paso 3: Cargar el Modelo en 4-bit\n",
        "print(f\"Paso 3: Cargando Modelo 4-bit desde {CFG.BASE_MODEL}...de Hugging Face\")\n",
        "model_start_time = time.time()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    CFG.BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False\n",
        ")\n",
        "model_end_time = time.time()\n",
        "print(f\"Modelo 4-bit cargado en {model_end_time - model_start_time:.2f} segundos.\")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"--- Proceso de carga completado en {end_time - start_time:.2f} segundos ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae090029",
      "metadata": {
        "id": "ae090029"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 6) Preparación PEFT (LoRA sobre QLoRA)\n",
        "#    [TRANSFORMER] Inyectamos adaptadores LoRA en q/k/v/o y MLP\n",
        "# ============================================================\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=CFG.LORA_R,\n",
        "    lora_alpha=CFG.LORA_ALPHA,\n",
        "    lora_dropout=CFG.LORA_DROPOUT,\n",
        "    target_modules=list(CFG.TARGET_MODULES),\n",
        "    task_type=CFG.TASK_TYPE,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb20f7ab",
      "metadata": {
        "id": "bb20f7ab"
      },
      "source": [
        "\n",
        "> **Alternativas PEFT**: IA3/AdaLoRA (cambia `peft_config`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2f31f4",
      "metadata": {
        "id": "8f2f31f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 7) Carga de Dataset (formato chat)\n",
        "# ============================================================\n",
        "from datasets import load_dataset, Dataset\n",
        "import json, os\n",
        "\n",
        "def load_chat_dataset(local_path: str | None, hf_id: str | None):\n",
        "    if local_path and os.path.exists(local_path):\n",
        "        with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            records = [json.loads(line) for line in f]\n",
        "        return Dataset.from_list(records)\n",
        "    elif hf_id:\n",
        "        return load_dataset(hf_id, split=\"train\")\n",
        "    else:\n",
        "        mini = [\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Compara API Gateway vs Service Mesh con pros/cons y cuándo usar cada uno.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"API Gateway gestiona tráfico norte-sur, auth, rate-limit; Mesh cubre este-oeste con mTLS, retries, observabilidad. Usa Gateway en el borde y Mesh intra-servicios cuando la malla sea compleja.\"}\n",
        "            ]},\n",
        "            {\"messages\": [\n",
        "                {\"role\":\"system\",\"content\":\"Eres un asistente experto en Arquitectura de Software.\"},\n",
        "                {\"role\":\"user\",\"content\":\"Diseña un patrón EDA en Kafka para fidelización al 99.99%.\"},\n",
        "                {\"role\":\"assistant\",\"content\":\"Particiones y RF≥3, acks=all, min.insync.replicas=2, DLQ, Schema Registry, idempotent producer, outbox, SLO/SLI y alertas por latencia/lag.\"}\n",
        "            ]},\n",
        "        ]\n",
        "        return Dataset.from_list(mini)\n",
        "\n",
        "raw_ds = load_chat_dataset(CFG.DATASET_LOCAL_JSONL, CFG.DATASET_HF_ID)\n",
        "print(\"Ejemplo:\", raw_ds[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf15fb0",
      "metadata": {
        "id": "fbf15fb0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# 8) Transformación de datos\n",
        "#    [DATA TRANSFORM] chat_template → tokenización → labels (pad→-100)\n",
        "# ============================================================\n",
        "def format_and_tokenize(example):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=CFG.MAX_SEQ_LEN,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    if input_ids and isinstance(input_ids[0], list):\n",
        "        labels = [\n",
        "            [tok if tok != pad_id else -100 for tok in seq]\n",
        "            for seq in input_ids\n",
        "        ]\n",
        "    else:\n",
        "        labels = [tok if tok != pad_id else -100 for tok in input_ids]\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "processed_ds = raw_ds.map(format_and_tokenize, remove_columns=raw_ds.column_names)\n",
        "split = processed_ds.train_test_split(test_size=0.05, seed=42)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6681e6e5",
      "metadata": {
        "id": "6681e6e5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9) Entrenamiento (TRL SFTTrainer)\n",
        "# ============================================================\n",
        "try:\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "except ModuleNotFoundError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    print(\"Instalando TRL (trl==0.11.4)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl==0.11.4\"])\n",
        "    from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "# Determine eval_strategy based on max_steps\n",
        "eval_strategy = \"steps\" if CFG.MAX_STEPS <= 0 else \"no\" # Disable step evaluation if max_steps is used\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=CFG.OUTPUT_DIR,\n",
        "    max_seq_length=CFG.MAX_SEQ_LEN,\n",
        "    per_device_train_batch_size=CFG.PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=CFG.GRADIENT_ACCUMULATION,\n",
        "    learning_rate=CFG.LEARNING_RATE,\n",
        "    logging_steps=CFG.LOGGING_STEPS,\n",
        "    eval_strategy=eval_strategy, # Use the determined strategy\n",
        "    eval_steps=CFG.EVAL_STEPS,\n",
        "    save_steps=CFG.SAVE_STEPS,\n",
        "    bf16=CFG.USE_BF16,\n",
        "    fp16=CFG.USE_FP16,\n",
        "    warmup_ratio=CFG.WARMUP_RATIO,\n",
        "    max_steps=CFG.MAX_STEPS if CFG.MAX_STEPS > 0 else -1, # Use -1 for no max steps\n",
        "    num_train_epochs=CFG.NUM_EPOCHS if CFG.MAX_STEPS <= 0 else 1000, # Set epochs to a large value if max_steps is used\n",
        "    gradient_checkpointing=CFG.GRADIENT_CHECKPOINTING,\n",
        "    report_to=[\"none\"],\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "#trainer.train()\n",
        "#trainer.model.save_pretrained(f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\")\n",
        "#tokenizer.save_pretrained(CFG.OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53c70f7",
      "metadata": {
        "id": "f53c70f7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10) Inferencia de prueba\n",
        "# ============================================================\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "def chat(prompt: str, sys: str = \"Eres un asistente experto en Arquitectura de Software.\"):\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\": sys},\n",
        "        {\"role\":\"user\",\"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # --- Agregar logs aquí ---\n",
        "    print(\"\\n--- Detalles de Inferencia ---\")\n",
        "    print(f\"Modelo base: {CFG.BASE_MODEL}\")\n",
        "    print(f\"Usando PEFT/LoRA: {isinstance(model, PeftModel)}\")\n",
        "    print(f\"Dispositivo del modelo: {model.device}\")\n",
        "    print(f\"Texto de entrada tokenizado: {text[:500]}...\") # Imprime los primeros 500 caracteres\n",
        "    print(f\"Longitud del texto de entrada: {len(text)}\")\n",
        "    # --------------------------\n",
        "\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # --- Más logs sobre los inputs ---\n",
        "    print(f\"Inputs tensor shape: {inputs['input_ids'].shape}\")\n",
        "    print(f\"Inputs tensor device: {inputs['input_ids'].device}\")\n",
        "    print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
        "    print(f\"Attention mask device: {inputs['attention_mask'].device}\")\n",
        "    print(f\"Max new tokens: {CFG.MAX_NEW_TOKENS}\")\n",
        "    print(f\"Temperature: {CFG.TEMPERATURE}\")\n",
        "    print(f\"Top P: {CFG.TOP_P}\")\n",
        "    print(\"-----------------------------\")\n",
        "    # -------------------------------\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
        "            temperature=CFG.TEMPERATURE,\n",
        "            top_p=CFG.TOP_P,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(chat(\"En terminos simples, explicame de que trata arquitectura dirigida por eventos EDA\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faeecd54",
      "metadata": {
        "id": "faeecd54"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11) (Opcional) Merge del adaptador y exportación\n",
        "# ============================================================\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import os\n",
        "import torch # Import torch\n",
        "\n",
        "# Define an offload directory (still needed if device_map is not used but model is large)\n",
        "offload_directory = \"/tmp/offload\"\n",
        "os.makedirs(offload_directory, exist_ok=True)\n",
        "\n",
        "# Determine device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "merged = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    f\"{CFG.OUTPUT_DIR}/{CFG.ADAPTER_NAME}\",\n",
        "    # device_map=\"auto\", # Remove auto device mapping\n",
        "    # Use explicit device if needed, or rely on default\n",
        "    offload_folder=offload_directory # Keep offload directory as a fallback/option\n",
        ").to(device) # Explicitly move to device\n",
        "\n",
        "merged = merged.merge_and_unload()\n",
        "merged.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a431da",
      "metadata": {
        "id": "c3a431da"
      },
      "source": [
        "\n",
        "## Troubleshooting (T4 + CUDA 12.6)\n",
        "- **bnb sin GPU / no lib cuda** → Repite la celda **0** y luego reinicia runtime. Verifica en **1)** que aparezca `libbitsandbytes_cuda126.so`.\n",
        "- **`bfloat16` no soportado** → Ya configurado (`USE_BF16=False`, `USE_FP16=True`).\n",
        "- **OOM** → Baja `MAX_SEQ_LEN` (1024→768/512), deja `BATCH=1`, mantén `GRADIENT_ACCUMULATION` alto, `gradient_checkpointing=True`.\n",
        "- **labels/pad** → Función de tokenización convierte PAD→`-100`.\n",
        "- **flash-attn/xformers** → Opcionales; SDPA de PyTorch es suficiente en T4.\n",
        "\n",
        "### Resumen didáctico\n",
        "- **[TRANSFORMER]**: celda **5** instancia `AutoModelForCausalLM` (Llama 3.x); **celda 6** inyecta LoRA en `q/k/v/o` y MLP.  \n",
        "- **[DATA TRANSFORM]**: celda **8** aplica `apply_chat_template` → `tokenizer` (trunc/pad) → `labels` (pad = -100).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "550cd272d56f4b23bbb2d2bd8d6e78ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6be13334b5b546268f6e3d00003b5e1e",
              "IPY_MODEL_5c38c7fd3a5c4b4b99a4ce8a84fcc1fa",
              "IPY_MODEL_5393acb40ea94b9d84942f49128707bf"
            ],
            "layout": "IPY_MODEL_f38c0e15f38742e2a03f4ad7b5f2785f"
          }
        },
        "6be13334b5b546268f6e3d00003b5e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2d818b378d46188bf2b98e03e54755",
            "placeholder": "​",
            "style": "IPY_MODEL_e64787c257124978b939fceff401595e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5c38c7fd3a5c4b4b99a4ce8a84fcc1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce3714788d2445cbaca5c65ef52180f1",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04dcff6a015646cf8464e449d9140fb3",
            "value": 4
          }
        },
        "5393acb40ea94b9d84942f49128707bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f9f233a3d2483f9bd2f3f7077d4d50",
            "placeholder": "​",
            "style": "IPY_MODEL_60ecae81ce4549a2bdc40287afdf4bb3",
            "value": " 4/4 [00:24&lt;00:00,  4.59s/it]"
          }
        },
        "f38c0e15f38742e2a03f4ad7b5f2785f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2d818b378d46188bf2b98e03e54755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e64787c257124978b939fceff401595e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce3714788d2445cbaca5c65ef52180f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04dcff6a015646cf8464e449d9140fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64f9f233a3d2483f9bd2f3f7077d4d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ecae81ce4549a2bdc40287afdf4bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}